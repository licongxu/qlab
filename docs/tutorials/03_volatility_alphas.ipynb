{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76accc0",
   "metadata": {},
   "source": [
    "# Tutorial 3 — Volatility-Based Alphas\n",
    "\n",
    "## Overview\n",
    "\n",
    "The **low-volatility anomaly** is one of the most puzzling findings in\n",
    "empirical asset pricing: stocks with *lower* volatility have historically\n",
    "delivered **higher risk-adjusted returns** than their high-volatility\n",
    "counterparts (Baker, Bradley & Wurgler, 2011).  This contradicts the\n",
    "textbook prediction that higher risk should be rewarded with higher return.\n",
    "\n",
    "Key explanations include **leverage constraints** (Frazzini & Pedersen, 2014),\n",
    "**lottery preference** (Bali, Cakici & Whitelaw, 2011), and\n",
    "**benchmarking incentives** that push managers toward high-beta stocks.\n",
    "\n",
    "This tutorial covers:\n",
    "1. Setup and data loading.\n",
    "2. The low-volatility anomaly — comparing four volatility estimators and backtesting.\n",
    "3. Betting against beta — BAB, idiosyncratic volatility, and comparative Sharpe ratios.\n",
    "4. Volatility timing — scaling a momentum strategy to target constant 10% vol.\n",
    "5. Risk-adjusted signals — the profitability proxy (rolling Sharpe ratio).\n",
    "6. What can go wrong — quintile transitions, sector bets, crowding, leverage.\n",
    "7. Summary table across all strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131fe200",
   "metadata": {},
   "source": [
    "## 1. Setup and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c207643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 5), \"figure.dpi\": 100,\n",
    "                     \"axes.grid\": True, \"grid.alpha\": 0.3})\n",
    "\n",
    "from qlab.data import YFinanceProvider, ParquetCache\n",
    "from qlab.features import (simple_returns, log_returns, rank, zscore, demean,\n",
    "                            realized_volatility, ewm_volatility,\n",
    "                            parkinson_volatility, garman_klass_volatility)\n",
    "from qlab.features.rolling import rolling_beta, rolling_mean, rolling_std\n",
    "from qlab.alphas import (low_volatility, idiosyncratic_vol, beta_signal,\n",
    "                          momentum, profitability_proxy)\n",
    "from qlab.portfolio import equal_weight_long_short, normalize_weights, proportional_weights\n",
    "from qlab.backtest import run_backtest, BacktestConfig\n",
    "from qlab.risk import performance_summary, drawdown_series, factor_regression, RegressionResult\n",
    "\n",
    "TICKERS = [\n",
    "    \"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"META\",\n",
    "    \"JPM\", \"GS\", \"BAC\",\n",
    "    \"JNJ\", \"PFE\", \"UNH\",\n",
    "    \"XOM\", \"CVX\",\n",
    "    \"PG\", \"KO\", \"WMT\",\n",
    "    \"HD\", \"NKE\",\n",
    "    \"CAT\", \"HON\",\n",
    "]\n",
    "START, END = \"2018-01-01\", \"2024-12-31\"\n",
    "\n",
    "provider = ParquetCache(YFinanceProvider(), cache_dir=\".qlab_cache\")\n",
    "prices = provider.fetch(TICKERS, START, END)\n",
    "close = prices[\"adj_close\"]\n",
    "print(f\"Universe : {close.index.get_level_values('ticker').nunique()} stocks\")\n",
    "print(f\"Date range: {close.index.get_level_values('date').min().date()} to \"\n",
    "      f\"{close.index.get_level_values('date').max().date()}\")\n",
    "print(f\"Total obs : {len(close):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281356b0",
   "metadata": {},
   "source": [
    "## 2. The low-volatility anomaly\n",
    "\n",
    "### Comparing four volatility estimators\n",
    "\n",
    "| Estimator | Inputs | Relative Efficiency |\n",
    "|-----------|--------|---------------------|\n",
    "| **Close-to-close** (realised) | Close prices | 1x (baseline) |\n",
    "| **EWM** (exponentially weighted) | Close prices | Reacts faster to recent data |\n",
    "| **Parkinson** (1980) | High, Low | ~5x more efficient than CC |\n",
    "| **Garman-Klass** (1980) | Open, High, Low, Close | ~8x more efficient than CC |\n",
    "\n",
    "All estimators are computed over a **21-day rolling window** and\n",
    "annualised by multiplying by $\\sqrt{252}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily log returns for close-close and EWM estimators\n",
    "rets = log_returns(close)\n",
    "\n",
    "# Four volatility estimators (21-day, annualised)\n",
    "vol_cc   = realized_volatility(rets, window=21, annualize=True)\n",
    "vol_ewm  = ewm_volatility(rets, halflife=21, annualize=True)\n",
    "vol_park = parkinson_volatility(prices[\"high\"], prices[\"low\"], window=21, annualize=True)\n",
    "vol_gk   = garman_klass_volatility(prices[\"open\"], prices[\"high\"],\n",
    "                                    prices[\"low\"], prices[\"close\"],\n",
    "                                    window=21, annualize=True)\n",
    "\n",
    "# Visual comparison for a single stock\n",
    "ticker = \"AAPL\"\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "for label, vol in [(\"Close-Close\", vol_cc), (\"EWM\", vol_ewm),\n",
    "                    (\"Parkinson\", vol_park), (\"Garman-Klass\", vol_gk)]:\n",
    "    ts = vol.xs(ticker, level=\"ticker\").dropna()\n",
    "    ax.plot(ts.index, ts.values, label=label, linewidth=1)\n",
    "ax.set_ylabel(\"Annualised Volatility\")\n",
    "ax.set_title(f\"Volatility Estimator Comparison — {ticker}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01098051",
   "metadata": {},
   "source": [
    "### Cross-sectional correlation heatmap between estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average cross-sectional rank correlation between all estimator pairs\n",
    "from qlab.utils.alignment import unstack_to_wide\n",
    "\n",
    "vol_dict = {\"CC\": vol_cc, \"EWM\": vol_ewm, \"Parkinson\": vol_park, \"GK\": vol_gk}\n",
    "vol_wide = {}\n",
    "for name, v in vol_dict.items():\n",
    "    vol_wide[name] = unstack_to_wide(v.dropna())\n",
    "\n",
    "# Find common dates and tickers across all estimators\n",
    "common_dates = vol_wide[\"CC\"].dropna(how=\"all\").index\n",
    "for name in vol_wide:\n",
    "    common_dates = common_dates.intersection(vol_wide[name].dropna(how=\"all\").index)\n",
    "common_tickers = vol_wide[\"CC\"].columns\n",
    "for name in vol_wide:\n",
    "    common_tickers = common_tickers.intersection(vol_wide[name].columns)\n",
    "\n",
    "estimator_names = list(vol_dict.keys())\n",
    "n_est = len(estimator_names)\n",
    "corr_matrix = np.ones((n_est, n_est))\n",
    "\n",
    "for i in range(n_est):\n",
    "    for j in range(i + 1, n_est):\n",
    "        corrs = []\n",
    "        w_i = vol_wide[estimator_names[i]].reindex(index=common_dates, columns=common_tickers)\n",
    "        w_j = vol_wide[estimator_names[j]].reindex(index=common_dates, columns=common_tickers)\n",
    "        for date in common_dates[::5]:  # sample every 5th date for speed\n",
    "            row_i = w_i.loc[date].dropna()\n",
    "            row_j = w_j.loc[date].dropna()\n",
    "            common_t = row_i.index.intersection(row_j.index)\n",
    "            if len(common_t) >= 5:\n",
    "                corrs.append(row_i[common_t].corr(row_j[common_t]))\n",
    "        avg_corr = np.nanmean(corrs) if corrs else np.nan\n",
    "        corr_matrix[i, j] = avg_corr\n",
    "        corr_matrix[j, i] = avg_corr\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(corr_matrix, vmin=0.5, vmax=1.0, cmap=\"YlOrRd\")\n",
    "ax.set_xticks(range(n_est))\n",
    "ax.set_xticklabels(estimator_names)\n",
    "ax.set_yticks(range(n_est))\n",
    "ax.set_yticklabels(estimator_names)\n",
    "for i in range(n_est):\n",
    "    for j in range(n_est):\n",
    "        ax.text(j, i, f\"{corr_matrix[i, j]:.2f}\", ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if corr_matrix[i, j] > 0.85 else \"black\", fontsize=12)\n",
    "fig.colorbar(im, ax=ax, label=\"Avg Cross-Sectional Rank Correlation\")\n",
    "ax.set_title(\"Correlation Heatmap — Volatility Estimators\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"All four estimators are highly correlated cross-sectionally.\")\n",
    "print(\"Parkinson and Garman-Klass capture intraday range information,\")\n",
    "print(\"making them more efficient (lower estimation error) than close-to-close.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1076880",
   "metadata": {},
   "source": [
    "### Low-volatility signal and backtest\n",
    "\n",
    "The `low_volatility` alpha returns **negative realised vol** (252-day window)\n",
    "so that lower-volatility stocks receive a higher score.  We build a quintile\n",
    "long/short portfolio: long the lowest-vol 20%, short the highest-vol 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149183a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-vol signal (252-day lookback, uses close-close vol internally)\n",
    "sig_lowvol = low_volatility(close, lookback=252)\n",
    "print(f\"Signal coverage: {sig_lowvol.dropna().index.get_level_values('date').nunique()} dates\")\n",
    "\n",
    "# Cross-section snapshot\n",
    "sample_date = sig_lowvol.dropna().index.get_level_values(\"date\").unique()[-100]\n",
    "cross = sig_lowvol.loc[sample_date].sort_values()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cross.plot.barh(ax=ax, color=[\"#d32f2f\" if v < cross.median() else \"#388e3c\" for v in cross.values])\n",
    "ax.set_xlabel(\"Low-Vol Signal (negative realised vol)\")\n",
    "ax.set_title(f\"Low-Vol Signal Cross-Section — {sample_date.date()}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48add50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio construction and backtest\n",
    "weights_lv = equal_weight_long_short(sig_lowvol.dropna(), long_pct=0.2, short_pct=0.2)\n",
    "weights_lv = normalize_weights(weights_lv, gross_exposure=2.0, net_exposure=0.0)\n",
    "\n",
    "config = BacktestConfig(\n",
    "    rebalance_freq=\"monthly\", commission_bps=5.0, slippage_bps=5.0,\n",
    "    signal_lag=1, execution_price=\"open\",\n",
    ")\n",
    "result_lv = run_backtest(weights_lv, prices, config=config)\n",
    "summary_lv = performance_summary(result_lv.portfolio_returns)\n",
    "\n",
    "print(\"Low-Volatility strategy backtest — real US equities:\")\n",
    "for k, v in summary_lv.items():\n",
    "    print(f\"  {k:25s}: {v:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a127b7",
   "metadata": {},
   "source": [
    "## 3. Betting against beta\n",
    "\n",
    "Frazzini & Pedersen (2014) show that *leverage-constrained* investors\n",
    "overweight high-beta stocks, pushing their prices above fair value.  The\n",
    "**Betting Against Beta (BAB)** factor goes long low-beta stocks and short\n",
    "high-beta stocks:\n",
    "\n",
    "$$\\text{BAB}_i = -\\beta_i$$\n",
    "\n",
    "We also compute **idiosyncratic volatility**: the volatility of residuals\n",
    "after removing market-beta exposure.  High idio-vol stocks tend to\n",
    "underperform (Ang, Hodrick, Xing & Zhang, 2006), likely driven by\n",
    "lottery-preference bias.\n",
    "\n",
    "| Signal | What it captures |\n",
    "|--------|-----------------|\n",
    "| Low-Vol | Total volatility (systematic + idiosyncratic) |\n",
    "| BAB | Systematic risk only (market beta) |\n",
    "| Idio-Vol | Residual risk after removing beta exposure |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406260a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market price proxy: equal-weighted average of our universe\n",
    "# This gives a Series indexed by date only (not MultiIndex)\n",
    "mkt_prices = close.groupby(level=\"date\").mean()\n",
    "print(f\"Market proxy: {len(mkt_prices)} dates, equal-weighted across {len(TICKERS)} stocks\")\n",
    "\n",
    "# BAB signal (negative beta)\n",
    "sig_bab = beta_signal(close, mkt_prices, lookback=252)\n",
    "\n",
    "# Idiosyncratic vol signal (negative idio vol)\n",
    "sig_idiovol = idiosyncratic_vol(close, mkt_prices, lookback=252)\n",
    "\n",
    "# Summary stats for all three signals\n",
    "for name, sig in [(\"BAB (neg beta)\", sig_bab),\n",
    "                   (\"Idio-Vol\", sig_idiovol),\n",
    "                   (\"Low-Vol\", sig_lowvol)]:\n",
    "    s = sig.dropna()\n",
    "    print(f\"{name:20s}: mean={s.mean():+.4f}  std={s.std():.4f}  \"\n",
    "          f\"coverage={s.index.get_level_values('date').nunique()} dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6777cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest all three volatility signals\n",
    "results_vol = {}\n",
    "summaries_vol = {}\n",
    "\n",
    "for name, sig in [(\"Low-Vol\", sig_lowvol),\n",
    "                   (\"BAB\", sig_bab),\n",
    "                   (\"Idio-Vol\", sig_idiovol)]:\n",
    "    w = equal_weight_long_short(sig.dropna(), long_pct=0.2, short_pct=0.2)\n",
    "    w = normalize_weights(w, gross_exposure=2.0, net_exposure=0.0)\n",
    "    res = run_backtest(w, prices, config=config)\n",
    "    results_vol[name] = res\n",
    "    summaries_vol[name] = performance_summary(res.portfolio_returns)\n",
    "\n",
    "# Equity curves\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "for name, res in results_vol.items():\n",
    "    cum = (1 + res.portfolio_returns).cumprod()\n",
    "    sr = summaries_vol[name][\"sharpe_ratio\"]\n",
    "    cum.plot(ax=ax, label=f\"{name} (SR={sr:.2f})\", linewidth=1.5)\n",
    "ax.axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.8)\n",
    "ax.set_ylabel(\"Cumulative Return\")\n",
    "ax.set_title(\"Volatility Signal Equity Curves — Quintile Long/Short\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4158cf",
   "metadata": {},
   "source": [
    "### Sharpe ratio comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparing Sharpe ratios across the three vol signals\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "names = list(summaries_vol.keys())\n",
    "sharpes = [summaries_vol[n][\"sharpe_ratio\"] for n in names]\n",
    "colors = [\"#2196F3\", \"#FF9800\", \"#4CAF50\"]\n",
    "bars = ax.bar(names, sharpes, color=colors, edgecolor=\"black\", linewidth=0.5)\n",
    "for bar, sr in zip(bars, sharpes):\n",
    "    ypos = sr + 0.02 if sr >= 0 else sr - 0.05\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, ypos,\n",
    "            f\"{sr:.2f}\", ha=\"center\", va=\"bottom\" if sr >= 0 else \"top\",\n",
    "            fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Sharpe Ratio\")\n",
    "ax.set_title(\"Volatility Signals — Sharpe Ratio Comparison\")\n",
    "ax.axhline(0, color=\"black\", linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print full comparison\n",
    "print(f\"{'Signal':<12s}  {'Ann.Ret':>8s}  {'Ann.Vol':>8s}  {'Sharpe':>8s}  {'MaxDD':>8s}\")\n",
    "print(\"-\" * 55)\n",
    "for name in names:\n",
    "    s = summaries_vol[name]\n",
    "    print(f\"{name:<12s}  {s['annualized_return']:>8.3f}  {s['annualized_volatility']:>8.3f}  \"\n",
    "          f\"{s['sharpe_ratio']:>8.3f}  {s['max_drawdown']:>8.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8686a",
   "metadata": {},
   "source": [
    "## 4. Volatility timing\n",
    "\n",
    "**Idea**: use volatility not as a cross-sectional signal, but as a\n",
    "**time-series portfolio scaler**.  Scale a strategy's weights by the\n",
    "inverse of its trailing realised volatility to target a constant\n",
    "annualised volatility (here 10%):\n",
    "\n",
    "$$w_t^{\\text{scaled}} = w_t^{\\text{raw}} \\times \\frac{\\sigma_{\\text{target}}}{\\hat{\\sigma}_t}$$\n",
    "\n",
    "When aggregate volatility is high (e.g., March 2020), reduce exposure.\n",
    "When volatility is low, scale up.  We use the **momentum** signal as\n",
    "the base strategy and compare raw vs vol-targeted equity curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d9b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base signal: 12-1 momentum\n",
    "sig_mom = momentum(close, lookback=252, skip=21)\n",
    "weights_mom = equal_weight_long_short(sig_mom.dropna(), long_pct=0.2, short_pct=0.2)\n",
    "weights_mom = normalize_weights(weights_mom, gross_exposure=2.0, net_exposure=0.0)\n",
    "\n",
    "# Raw momentum backtest\n",
    "result_mom_raw = run_backtest(weights_mom, prices, config=config)\n",
    "\n",
    "# Compute trailing 63-day portfolio vol (annualised)\n",
    "port_ret_raw = result_mom_raw.portfolio_returns\n",
    "trailing_vol = port_ret_raw.rolling(63, min_periods=21).std() * np.sqrt(252)\n",
    "\n",
    "# Vol-target: scale returns by (target_vol / trailing_vol)\n",
    "# In practice you would rescale weights before execution; here we scale\n",
    "# returns for a cleaner illustration of the vol-targeting effect.\n",
    "VOL_TARGET = 0.10  # 10% annualised\n",
    "vol_scalar = VOL_TARGET / trailing_vol.clip(lower=0.01)\n",
    "vol_scalar = vol_scalar.clip(upper=3.0)  # cap leverage at 3x\n",
    "\n",
    "port_ret_voltgt = port_ret_raw * vol_scalar.shift(1)  # use prior day's scalar\n",
    "port_ret_voltgt = port_ret_voltgt.dropna()\n",
    "\n",
    "print(f\"Raw momentum realised vol:   {port_ret_raw.std() * np.sqrt(252):.3f}\")\n",
    "print(f\"Vol-targeted realised vol:   {port_ret_voltgt.std() * np.sqrt(252):.3f}\")\n",
    "print(f\"Target vol:                  {VOL_TARGET:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47461861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare equity curves: raw momentum vs vol-targeted\n",
    "cum_raw = (1 + port_ret_raw).cumprod()\n",
    "cum_voltgt = (1 + port_ret_voltgt).cumprod()\n",
    "\n",
    "sr_raw = performance_summary(port_ret_raw)[\"sharpe_ratio\"]\n",
    "sr_voltgt = performance_summary(port_ret_voltgt)[\"sharpe_ratio\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Equity curves\n",
    "cum_raw.plot(ax=ax1, label=f\"Raw Momentum (SR={sr_raw:.2f})\", color=\"navy\", linewidth=1.5)\n",
    "cum_voltgt.plot(ax=ax1, label=f\"Vol-Targeted 10% (SR={sr_voltgt:.2f})\", color=\"darkorange\", linewidth=1.5)\n",
    "ax1.axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.8)\n",
    "ax1.set_ylabel(\"Cumulative Return\")\n",
    "ax1.set_title(\"Momentum: Raw vs Vol-Targeted Equity Curves\")\n",
    "ax1.legend()\n",
    "\n",
    "# Volatility scaling factor over time\n",
    "vol_scalar.plot(ax=ax2, color=\"steelblue\", linewidth=1)\n",
    "ax2.axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.8)\n",
    "ax2.set_ylabel(\"Vol Scalar\")\n",
    "ax2.set_title(\"Volatility Scaling Factor Over Time (capped at 3x)\")\n",
    "ax2.set_xlabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRaw momentum performance:\")\n",
    "for k, v in performance_summary(port_ret_raw).items():\n",
    "    print(f\"  {k:25s}: {v:>10.4f}\")\n",
    "print(f\"\\nVol-targeted performance:\")\n",
    "for k, v in performance_summary(port_ret_voltgt).items():\n",
    "    print(f\"  {k:25s}: {v:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01c56d",
   "metadata": {},
   "source": [
    "## 5. Risk-adjusted signals\n",
    "\n",
    "The `profitability_proxy` signal computes a **rolling Sharpe ratio** per\n",
    "stock:\n",
    "\n",
    "$$\\text{Quality}_i = \\frac{\\bar{r}_i}{\\sigma_i}$$\n",
    "\n",
    "over a trailing 252-day window.  Stocks with high risk-adjusted returns\n",
    "are treated as higher-quality names.\n",
    "\n",
    "This is a cross between a momentum signal (rewarding high $\\bar{r}$) and\n",
    "a low-vol signal (penalising high $\\sigma$).  It selects consistent,\n",
    "low-variance winners rather than rewarding raw performance alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5aa962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profitability proxy signal\n",
    "sig_prof = profitability_proxy(close, lookback=252)\n",
    "\n",
    "# Cross-sectional rank correlations with momentum and low-vol\n",
    "from qlab.features.cross_section import rank as cs_rank\n",
    "\n",
    "r_prof = cs_rank(sig_prof.dropna())\n",
    "r_mom = cs_rank(sig_mom.dropna())\n",
    "r_lowvol = cs_rank(sig_lowvol.dropna())\n",
    "\n",
    "# Align on common index\n",
    "common_idx = r_prof.index.intersection(r_mom.index).intersection(r_lowvol.index)\n",
    "r_prof_c = r_prof.reindex(common_idx)\n",
    "r_mom_c = r_mom.reindex(common_idx)\n",
    "r_lowvol_c = r_lowvol.reindex(common_idx)\n",
    "\n",
    "# Average daily cross-sectional rank correlations\n",
    "corr_prof_mom = r_prof_c.groupby(level=\"date\").corr(r_mom_c).mean()\n",
    "corr_prof_lv = r_prof_c.groupby(level=\"date\").corr(r_lowvol_c).mean()\n",
    "corr_mom_lv = r_mom_c.groupby(level=\"date\").corr(r_lowvol_c).mean()\n",
    "\n",
    "print(\"Average cross-sectional rank correlations:\")\n",
    "print(f\"  Profitability vs Momentum:    {corr_prof_mom:.3f}\")\n",
    "print(f\"  Profitability vs Low-Vol:     {corr_prof_lv:.3f}\")\n",
    "print(f\"  Momentum vs Low-Vol:          {corr_mom_lv:.3f}\")\n",
    "print()\n",
    "print(\"Profitability is a blend: positively correlated with both momentum\")\n",
    "print(\"and low-vol, but penalises noisy winners and rewards steady compounders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dccdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest profitability proxy\n",
    "weights_prof = equal_weight_long_short(sig_prof.dropna(), long_pct=0.2, short_pct=0.2)\n",
    "weights_prof = normalize_weights(weights_prof, gross_exposure=2.0, net_exposure=0.0)\n",
    "result_prof = run_backtest(weights_prof, prices, config=config)\n",
    "summary_prof = performance_summary(result_prof.portfolio_returns)\n",
    "\n",
    "# Equity curve with drawdown\n",
    "cum_prof = (1 + result_prof.portfolio_returns).cumprod()\n",
    "dd_prof = drawdown_series(result_prof.portfolio_returns)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "cum_prof.plot(ax=ax1, color=\"teal\", linewidth=1.5)\n",
    "ax1.axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.8)\n",
    "ax1.set_ylabel(\"Cumulative Return\")\n",
    "ax1.set_title(f\"Profitability Proxy — Equity Curve (SR={summary_prof['sharpe_ratio']:.2f})\")\n",
    "\n",
    "dd_prof.plot(ax=ax2, color=\"crimson\", linewidth=1)\n",
    "ax2.fill_between(dd_prof.index, dd_prof.values, 0, alpha=0.3, color=\"crimson\")\n",
    "ax2.set_ylabel(\"Drawdown\")\n",
    "ax2.set_title(\"Underwater Chart\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Profitability proxy backtest:\")\n",
    "for k, v in summary_prof.items():\n",
    "    print(f\"  {k:25s}: {v:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937cf7fd",
   "metadata": {},
   "source": [
    "## 6. What can go wrong\n",
    "\n",
    "### Volatility quintile transition matrix\n",
    "\n",
    "How stable are volatility rankings?  If stocks frequently jump between\n",
    "quintiles, the signal has poor persistence and turnover will be high.\n",
    "The transition matrix below shows the probability of moving from one\n",
    "volatility quintile to another over a one-month horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673edffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign quintile labels (1 = lowest vol, 5 = highest vol) using 252-day realised vol\n",
    "vol_252 = realized_volatility(rets, window=252, annualize=True).dropna()\n",
    "\n",
    "def assign_quintile(group):\n",
    "    try:\n",
    "        return pd.qcut(group.rank(method=\"first\"), 5, labels=[1, 2, 3, 4, 5]).astype(int)\n",
    "    except ValueError:\n",
    "        return pd.Series(np.nan, index=group.index)\n",
    "\n",
    "quintiles = vol_252.groupby(level=\"date\").transform(assign_quintile)\n",
    "\n",
    "# Monthly transition matrix: compare quintile at month-end to quintile ~21 days later\n",
    "dates = quintiles.dropna().index.get_level_values(\"date\").unique().sort_values()\n",
    "monthly_dates = dates[::21]  # sample approximately monthly\n",
    "\n",
    "transitions = []\n",
    "for i in range(len(monthly_dates) - 1):\n",
    "    d0, d1 = monthly_dates[i], monthly_dates[i + 1]\n",
    "    try:\n",
    "        q0 = quintiles.loc[d0]\n",
    "        q1 = quintiles.loc[d1]\n",
    "        common = q0.dropna().index.intersection(q1.dropna().index)\n",
    "        if len(common) >= 10:\n",
    "            for t in common:\n",
    "                transitions.append((int(q0.loc[t]), int(q1.loc[t])))\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "trans_df = pd.DataFrame(transitions, columns=[\"from_q\", \"to_q\"])\n",
    "trans_matrix = pd.crosstab(trans_df[\"from_q\"], trans_df[\"to_q\"], normalize=\"index\")\n",
    "trans_matrix.index.name = \"From Quintile\"\n",
    "trans_matrix.columns.name = \"To Quintile\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "im = ax.imshow(trans_matrix.values, cmap=\"Blues\", vmin=0, vmax=0.6)\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        val = trans_matrix.values[i, j]\n",
    "        ax.text(j, i, f\"{val:.1%}\", ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if val > 0.35 else \"black\", fontsize=11)\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels([1, 2, 3, 4, 5])\n",
    "ax.set_yticks(range(5))\n",
    "ax.set_yticklabels([1, 2, 3, 4, 5])\n",
    "ax.set_xlabel(\"To Quintile (next month)\")\n",
    "ax.set_ylabel(\"From Quintile (current month)\")\n",
    "ax.set_title(\"Volatility Quintile Transition Matrix (monthly)\")\n",
    "fig.colorbar(im, ax=ax, label=\"Transition Probability\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "diag_avg = np.diag(trans_matrix.values).mean()\n",
    "print(f\"Diagonal average (persistence): {diag_avg:.1%}\")\n",
    "print(\"Higher diagonal = more persistent vol ranking = lower turnover cost.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf4594",
   "metadata": {},
   "source": [
    "### Key risks for volatility-based strategies\n",
    "\n",
    "**Sector concentration**: Low-volatility portfolios tend to overweight\n",
    "defensive sectors (utilities, consumer staples, healthcare) and underweight\n",
    "cyclicals (technology, energy).  These unintended sector bets can dominate\n",
    "returns, particularly during style rotations.\n",
    "\n",
    "**Crowding**: The low-vol anomaly has attracted significant capital since\n",
    "publication, especially through \"smart beta\" ETFs.  Crowded trades create\n",
    "fragility: when investors unwind simultaneously, low-vol stocks experience\n",
    "sharp drawdowns (as seen in late 2020 during the growth/value rotation).\n",
    "\n",
    "**Leverage constraints**: BAB in its pure form requires shorting high-beta\n",
    "stocks and leveraging low-beta stocks.  In practice, margin requirements,\n",
    "short-borrowing costs, and funding constraints erode the theoretical edge.\n",
    "\n",
    "**Rate sensitivity**: Low-volatility stocks are often \"bond proxies\" with\n",
    "high duration.  Rising interest rates (e.g., 2022) can cause coordinated\n",
    "losses across the low-vol universe, overwhelming the cross-sectional signal.\n",
    "\n",
    "**Rebalancing costs**: The transition matrix above shows moderate quintile\n",
    "persistence.  Frequent rebalancing adds transaction costs; infrequent\n",
    "rebalancing lets the portfolio drift from target exposures.  Monthly\n",
    "rebalancing is a reasonable compromise for volatility signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbcf199",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile summary table across all strategies tested in this tutorial\n",
    "all_strategies = {\n",
    "    \"Low-Vol\": result_lv,\n",
    "    \"BAB\": results_vol[\"BAB\"],\n",
    "    \"Idio-Vol\": results_vol[\"Idio-Vol\"],\n",
    "    \"Profitability\": result_prof,\n",
    "    \"Momentum (raw)\": result_mom_raw,\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, res in all_strategies.items():\n",
    "    s = performance_summary(res.portfolio_returns)\n",
    "    rows.append({\n",
    "        \"Strategy\": name,\n",
    "        \"Ann. Return\": f\"{s['annualized_return']:.2%}\",\n",
    "        \"Ann. Vol\": f\"{s['annualized_volatility']:.2%}\",\n",
    "        \"Sharpe\": f\"{s['sharpe_ratio']:.2f}\",\n",
    "        \"Sortino\": f\"{s['sortino_ratio']:.2f}\",\n",
    "        \"Max DD\": f\"{s['max_drawdown']:.2%}\",\n",
    "        \"Calmar\": f\"{s['calmar_ratio']:.2f}\",\n",
    "        \"Hit Rate\": f\"{s['hit_rate']:.1%}\",\n",
    "    })\n",
    "\n",
    "summary_table = pd.DataFrame(rows).set_index(\"Strategy\")\n",
    "print(\"=== Strategy Comparison ===\")\n",
    "print(summary_table.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0209d",
   "metadata": {},
   "source": [
    "### Key takeaways\n",
    "\n",
    "| Concept | Takeaway |\n",
    "|---------|----------|\n",
    "| Low-vol anomaly | Lower-risk stocks have historically earned higher risk-adjusted returns |\n",
    "| Vol estimators | Parkinson and GK are more efficient, but all are highly correlated cross-sectionally |\n",
    "| BAB | Betting against beta isolates the leverage-constraint channel; requires actual leverage |\n",
    "| Idio-vol | Removing market exposure isolates stock-specific risk; captures lottery preference |\n",
    "| Vol timing | Scaling by inverse trailing vol stabilises equity curves and can improve Sharpe |\n",
    "| Profitability proxy | Rolling Sharpe ratio blends momentum and low-vol into a quality-like signal |\n",
    "| Risks | Sector concentration, crowding, rate sensitivity, leverage constraints |\n",
    "\n",
    "The next tutorial shows how to **combine** momentum, mean reversion, and\n",
    "volatility signals into a multi-factor composite."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cmbagent_env)",
   "language": "python",
   "name": "cmbagent_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
