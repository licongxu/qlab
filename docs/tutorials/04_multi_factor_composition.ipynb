{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a746041",
   "metadata": {},
   "source": [
    "# Tutorial 4 — Multi-Factor Alpha Composition\n",
    "\n",
    "## Overview\n",
    "\n",
    "Individual alpha signals — momentum, reversal, low-volatility, quality —\n",
    "each capture a different dimension of cross-sectional return predictability.\n",
    "No single signal dominates in all market regimes.\n",
    "\n",
    "Combining signals into a **composite alpha** can:\n",
    "- **Diversify** across signal-specific risk (a momentum crash does not coincide with a vol regime shift).\n",
    "- **Stabilise** performance across market regimes.\n",
    "- **Increase capacity** by spreading bets more evenly across the universe.\n",
    "\n",
    "This tutorial covers:\n",
    "1. Signal computation and normalisation.\n",
    "2. Cross-sectional signal correlation analysis.\n",
    "3. Three combination methods: equal-weight, IC-weighted, rank-then-combine.\n",
    "4. Backtesting all individual and composite strategies.\n",
    "5. Marginal contribution analysis (leave-one-out).\n",
    "6. Correlation stability over time.\n",
    "7. What can go wrong — overfitting, common exposures, correlation breakdown.\n",
    "8. Summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a70b7",
   "metadata": {},
   "source": [
    "## 1. Setup and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 5), \"figure.dpi\": 100,\n",
    "                     \"axes.grid\": True, \"grid.alpha\": 0.3})\n",
    "\n",
    "from qlab.data import YFinanceProvider, ParquetCache\n",
    "from qlab.features import simple_returns, rank, zscore, winsorize, demean, realized_volatility\n",
    "from qlab.alphas import momentum, short_term_reversal, low_volatility, profitability_proxy\n",
    "from qlab.portfolio import equal_weight_long_short, proportional_weights, normalize_weights, apply_position_limits\n",
    "from qlab.backtest import run_backtest, BacktestConfig\n",
    "from qlab.risk import performance_summary, drawdown_series, factor_regression\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "TICKERS = [\n",
    "    \"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"META\",\n",
    "    \"JPM\", \"GS\", \"BAC\",\n",
    "    \"JNJ\", \"PFE\", \"UNH\",\n",
    "    \"XOM\", \"CVX\",\n",
    "    \"PG\", \"KO\", \"WMT\",\n",
    "    \"HD\", \"NKE\",\n",
    "    \"CAT\", \"HON\",\n",
    "]\n",
    "START, END = \"2018-01-01\", \"2024-12-31\"\n",
    "\n",
    "provider = ParquetCache(YFinanceProvider(), cache_dir=\".qlab_cache\")\n",
    "prices = provider.fetch(TICKERS, START, END)\n",
    "close = prices[\"adj_close\"]\n",
    "print(f\"Universe : {close.index.get_level_values('ticker').nunique()} stocks\")\n",
    "print(f\"Date range: {close.index.get_level_values('date').min().date()} to \"\n",
    "      f\"{close.index.get_level_values('date').max().date()}\")\n",
    "print(f\"Total obs : {len(close):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a4684",
   "metadata": {},
   "source": [
    "## 2. Compute individual alpha signals\n",
    "\n",
    "We construct four signals spanning different return drivers:\n",
    "\n",
    "| Signal | Function | Lookback | Intuition |\n",
    "|--------|----------|----------|-----------|\n",
    "| **Momentum** | `momentum(close, 252, 21)` | 12 months, skip 1 month | Winners keep winning |\n",
    "| **Short-term reversal** | `short_term_reversal(close, 21)` | 1 month | Recent losers bounce back |\n",
    "| **Low volatility** | `low_volatility(close, 126)` | 6 months | Calm stocks outperform |\n",
    "| **Profitability proxy** | `profitability_proxy(close, 252)` | 12 months | High risk-adjusted return signals quality |\n",
    "\n",
    "Each raw signal is normalised via `zscore(winsorize(...))` to put them on a\n",
    "comparable scale and tame outliers before combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1953a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute raw signals\n",
    "sig_mom_raw = momentum(close, lookback=252, skip=21)\n",
    "sig_rev_raw = short_term_reversal(close, lookback=21)\n",
    "sig_vol_raw = low_volatility(close, lookback=126)\n",
    "sig_prof_raw = profitability_proxy(close, lookback=252)\n",
    "\n",
    "# Normalise: winsorise at 5th/95th percentile, then cross-sectional z-score\n",
    "sig_mom = zscore(winsorize(sig_mom_raw.dropna(), lower=0.05, upper=0.95))\n",
    "sig_rev = zscore(winsorize(sig_rev_raw.dropna(), lower=0.05, upper=0.95))\n",
    "sig_vol = zscore(winsorize(sig_vol_raw.dropna(), lower=0.05, upper=0.95))\n",
    "sig_prof = zscore(winsorize(sig_prof_raw.dropna(), lower=0.05, upper=0.95))\n",
    "\n",
    "# Align all signals on a common (date, ticker) index\n",
    "signals = {\"Momentum\": sig_mom, \"Reversal\": sig_rev,\n",
    "           \"LowVol\": sig_vol, \"Profitability\": sig_prof}\n",
    "common_idx = sig_mom.dropna().index\n",
    "for s in signals.values():\n",
    "    common_idx = common_idx.intersection(s.dropna().index)\n",
    "\n",
    "signals = {name: sig.reindex(common_idx) for name, sig in signals.items()}\n",
    "sig_df = pd.DataFrame(signals)\n",
    "\n",
    "print(f\"Common index: {sig_df.index.get_level_values('date').nunique()} dates, \"\n",
    "      f\"{sig_df.index.get_level_values('ticker').nunique()} tickers, \"\n",
    "      f\"{len(sig_df):,} observations\")\n",
    "print(f\"Date range: {sig_df.index.get_level_values('date').min().date()} to \"\n",
    "      f\"{sig_df.index.get_level_values('date').max().date()}\")\n",
    "print(\"\\nSignal summary statistics:\")\n",
    "print(sig_df.describe().round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the four signals on a sample date\n",
    "sample_date = sig_df.index.get_level_values(\"date\").unique()[\n",
    "    len(sig_df.index.get_level_values(\"date\").unique()) // 2\n",
    "]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "for ax, (name, sig) in zip(axes.flat, signals.items()):\n",
    "    vals = sig.loc[sample_date].sort_values()\n",
    "    colors = [\"#d32f2f\" if v < 0 else \"#388e3c\" for v in vals.values]\n",
    "    vals.plot.barh(ax=ax, color=colors)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"Normalised signal\")\n",
    "fig.suptitle(f\"Four alpha signals on {sample_date.date()}\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31ac6d",
   "metadata": {},
   "source": [
    "## 3. Signal correlation analysis\n",
    "\n",
    "Low correlation between signals is the prerequisite for diversification.\n",
    "We compute the **average cross-sectional Spearman rank correlation** between\n",
    "all signal pairs.\n",
    "\n",
    "| Correlation range | Implication |\n",
    "|-------------------|-------------|\n",
    "| > 0.5 | Highly redundant; combining adds little |\n",
    "| 0.1 to 0.5 | Moderate overlap; combination beneficial |\n",
    "| -0.3 to 0.1 | Low/negative; excellent diversification |\n",
    "| < -0.3 | Strongly opposing; check for cancellation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_names = list(signals.keys())\n",
    "n_signals = len(signal_names)\n",
    "dates = sig_df.index.get_level_values(\"date\").unique()\n",
    "\n",
    "# Compute average cross-sectional Spearman correlation between all pairs\n",
    "corr_matrix = np.zeros((n_signals, n_signals))\n",
    "for i in range(n_signals):\n",
    "    for j in range(n_signals):\n",
    "        if i == j:\n",
    "            corr_matrix[i, j] = 1.0\n",
    "            continue\n",
    "        corrs = []\n",
    "        for d in dates[::5]:  # sample every 5th date for speed\n",
    "            s_i = signals[signal_names[i]].loc[d].dropna()\n",
    "            s_j = signals[signal_names[j]].loc[d].dropna()\n",
    "            common_tickers = s_i.index.intersection(s_j.index)\n",
    "            if len(common_tickers) >= 5:\n",
    "                rho, _ = spearmanr(s_i.loc[common_tickers].values,\n",
    "                                   s_j.loc[common_tickers].values)\n",
    "                if not np.isnan(rho):\n",
    "                    corrs.append(rho)\n",
    "        corr_matrix[i, j] = np.mean(corrs) if corrs else 0.0\n",
    "\n",
    "corr_df = pd.DataFrame(corr_matrix, index=signal_names, columns=signal_names)\n",
    "print(\"Average cross-sectional Spearman correlation:\")\n",
    "print(corr_df.round(3).to_string())\n",
    "print(f\"\\nAverage off-diagonal correlation: \"\n",
    "      f\"{(corr_matrix.sum() - n_signals) / (n_signals * (n_signals - 1)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap with annotated values\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "im = ax.imshow(corr_matrix, cmap=\"RdBu_r\", vmin=-1, vmax=1, aspect=\"auto\")\n",
    "ax.set_xticks(range(n_signals))\n",
    "ax.set_xticklabels(signal_names, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(range(n_signals))\n",
    "ax.set_yticklabels(signal_names)\n",
    "\n",
    "for i in range(n_signals):\n",
    "    for j in range(n_signals):\n",
    "        color = \"white\" if abs(corr_matrix[i, j]) > 0.5 else \"black\"\n",
    "        ax.text(j, i, f\"{corr_matrix[i, j]:.2f}\",\n",
    "                ha=\"center\", va=\"center\", color=color, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "plt.colorbar(im, ax=ax, label=\"Spearman correlation\")\n",
    "ax.set_title(\"Average Cross-Sectional Signal Correlations\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f5b8c",
   "metadata": {},
   "source": [
    "## 4. Combination methods\n",
    "\n",
    "### Method A — Equal-weight z-score average\n",
    "\n",
    "The simplest approach: average the normalised signals with equal weight.\n",
    "No look-ahead bias, fully out-of-sample. This is the baseline that any\n",
    "fancier method must beat.\n",
    "\n",
    "$$\\alpha_i^{\\text{EW}} = \\frac{1}{K} \\sum_{k=1}^{K} z_{ik}$$\n",
    "\n",
    "### Method B — IC-weighted average\n",
    "\n",
    "Weight each signal by its trailing **Information Coefficient** (cross-sectional\n",
    "Spearman rank correlation with 21-day forward returns). Signals with higher\n",
    "predictive power get more weight. We shrink the IC weights 50% toward equal\n",
    "weight to reduce estimation noise.\n",
    "\n",
    "### Method C — Rank-then-combine\n",
    "\n",
    "Rank each signal into cross-sectional percentiles first, then average the\n",
    "ranks. This is robust to outliers and ensures each signal contributes\n",
    "equally regardless of distribution shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe774607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Method A: Equal-weight average ──\n",
    "combo_ew = sig_df.mean(axis=1)\n",
    "combo_ew = zscore(combo_ew.dropna())\n",
    "combo_ew.name = \"EqualWeight\"\n",
    "\n",
    "print(\"Method A: Equal-weight composite\")\n",
    "print(f\"  Observations: {len(combo_ew.dropna()):,}\")\n",
    "print(f\"  Cross-sectional std (mean): {combo_ew.groupby(level='date').std().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7cf036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Method B: IC-weighted combination ──\n",
    "# Compute 21-day forward returns for IC calculation\n",
    "fwd_ret = simple_returns(close, periods=21).groupby(level=\"ticker\").shift(-21)\n",
    "fwd_ret = fwd_ret.reindex(common_idx)\n",
    "\n",
    "# Compute trailing IC per signal using a 126-day expanding window\n",
    "# Sample every 5th date, subsample trailing dates for efficiency\n",
    "ic_window = 126\n",
    "ic_series = {name: [] for name in signal_names}\n",
    "ic_date_list = []\n",
    "\n",
    "for d_idx in range(ic_window, len(dates), 5):\n",
    "    d = dates[d_idx]\n",
    "    trail_dates = dates[max(0, d_idx - ic_window):d_idx]\n",
    "    for name in signal_names:\n",
    "        ics = []\n",
    "        for td in trail_dates[::3]:\n",
    "            try:\n",
    "                sig_d = signals[name].loc[td].dropna()\n",
    "                fwd_d = fwd_ret.loc[td].dropna()\n",
    "                common_t = sig_d.index.intersection(fwd_d.index)\n",
    "                if len(common_t) >= 5:\n",
    "                    rho, _ = spearmanr(sig_d.loc[common_t].values,\n",
    "                                       fwd_d.loc[common_t].values)\n",
    "                    if not np.isnan(rho):\n",
    "                        ics.append(rho)\n",
    "            except Exception:\n",
    "                continue\n",
    "        ic_series[name].append(np.mean(ics) if ics else 0.0)\n",
    "    ic_date_list.append(d)\n",
    "\n",
    "ic_df = pd.DataFrame(ic_series, index=ic_date_list)\n",
    "print(\"Trailing IC statistics:\")\n",
    "print(ic_df.describe().round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6350c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build IC-weighted composite with 50% shrinkage toward equal weight\n",
    "shrinkage = 0.5\n",
    "equal_w = 1.0 / n_signals\n",
    "\n",
    "# Interpolate IC weights to all dates, then shrink\n",
    "ic_weights_full = ic_df.reindex(dates).interpolate(method=\"index\").ffill().bfill()\n",
    "# Clip negative ICs to zero before weighting\n",
    "ic_weights_pos = ic_weights_full.clip(lower=0)\n",
    "# Normalise so weights sum to 1 (or equal weight if all zero)\n",
    "row_sums = ic_weights_pos.sum(axis=1)\n",
    "ic_weights_norm = ic_weights_pos.div(row_sums.replace(0, np.nan), axis=0).fillna(equal_w)\n",
    "# Shrink toward equal weight\n",
    "ic_weights_shrunk = shrinkage * equal_w + (1 - shrinkage) * ic_weights_norm\n",
    "\n",
    "# Build composite signal date by date\n",
    "combo_ic_parts = []\n",
    "for d in dates:\n",
    "    if d not in ic_weights_shrunk.index:\n",
    "        continue\n",
    "    w = ic_weights_shrunk.loc[d]\n",
    "    if d in sig_df.index.get_level_values(\"date\"):\n",
    "        sig_d = sig_df.loc[d]\n",
    "        if not sig_d.empty:\n",
    "            combined = (sig_d * w.values).sum(axis=1)\n",
    "            combo_ic_parts.append(combined)\n",
    "\n",
    "combo_ic = pd.concat(combo_ic_parts)\n",
    "combo_ic = zscore(combo_ic.dropna())\n",
    "combo_ic.name = \"ICWeighted\"\n",
    "\n",
    "print(\"Method B: IC-weighted composite (50% shrinkage)\")\n",
    "print(f\"  Observations: {len(combo_ic.dropna()):,}\")\n",
    "print(f\"\\nShrunk IC weights (last available date):\")\n",
    "last_w = ic_weights_shrunk.iloc[-1]\n",
    "for name, w in last_w.items():\n",
    "    print(f\"  {name:15s}: {w:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Method C: Rank-then-combine ──\n",
    "ranked_signals = {name: rank(sig) for name, sig in signals.items()}\n",
    "rank_df = pd.DataFrame(ranked_signals)\n",
    "combo_rank = rank_df.mean(axis=1)\n",
    "combo_rank = zscore(combo_rank.dropna())\n",
    "combo_rank.name = \"RankCombo\"\n",
    "\n",
    "print(\"Method C: Rank-then-combine composite\")\n",
    "print(f\"  Observations: {len(combo_rank.dropna()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b58df54",
   "metadata": {},
   "source": [
    "## 5. Backtest all strategies\n",
    "\n",
    "We backtest all 7 strategies (4 individual + 3 composite) with identical\n",
    "portfolio construction: quintile long/short, dollar neutral, capped at 10%\n",
    "per position, monthly rebalance, 5 bps commission + 5 bps slippage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bed46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_signals = {\n",
    "    \"Momentum\": signals[\"Momentum\"],\n",
    "    \"Reversal\": signals[\"Reversal\"],\n",
    "    \"LowVol\": signals[\"LowVol\"],\n",
    "    \"Profitability\": signals[\"Profitability\"],\n",
    "    \"EqualWeight\": combo_ew,\n",
    "    \"ICWeighted\": combo_ic,\n",
    "    \"RankCombo\": combo_rank,\n",
    "}\n",
    "\n",
    "config = BacktestConfig(\n",
    "    rebalance_freq=\"monthly\", commission_bps=5.0, slippage_bps=5.0,\n",
    "    signal_lag=1, execution_price=\"open\",\n",
    ")\n",
    "\n",
    "results = {}\n",
    "summaries = {}\n",
    "\n",
    "for name, sig in all_signals.items():\n",
    "    sig_clean = sig.dropna()\n",
    "    w = equal_weight_long_short(sig_clean, long_pct=0.2, short_pct=0.2)\n",
    "    w = normalize_weights(w, gross_exposure=2.0, net_exposure=0.0)\n",
    "    w = apply_position_limits(w, max_weight=0.10, min_weight=-0.10)\n",
    "    res = run_backtest(w, prices, config=config)\n",
    "    results[name] = res\n",
    "    summaries[name] = performance_summary(res.portfolio_returns)\n",
    "    print(f\"  {name:15s}: Sharpe={summaries[name]['sharpe_ratio']:+.3f}  \"\n",
    "          f\"AnnRet={summaries[name]['annualized_return']:+.3%}  \"\n",
    "          f\"MaxDD={summaries[name]['max_drawdown']:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fca948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary_df = pd.DataFrame(summaries).T\n",
    "summary_df = summary_df[[\"annualized_return\", \"annualized_volatility\", \"sharpe_ratio\",\n",
    "                          \"sortino_ratio\", \"max_drawdown\", \"calmar_ratio\", \"hit_rate\"]]\n",
    "summary_df.columns = [\"Ann. Return\", \"Ann. Vol\", \"Sharpe\", \"Sortino\",\n",
    "                       \"Max DD\", \"Calmar\", \"Hit Rate\"]\n",
    "summary_df = summary_df.round(4)\n",
    "summary_df[\"Type\"] = ([\"Individual\"] * 4) + ([\"Composite\"] * 3)\n",
    "\n",
    "print(\"Performance Summary - All Strategies\")\n",
    "print(\"=\" * 95)\n",
    "print(summary_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216152f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equity curves: individual strategies (top) and composites (bottom)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "colors_ind = {\"Momentum\": \"#1f77b4\", \"Reversal\": \"#ff7f0e\",\n",
    "              \"LowVol\": \"#2ca02c\", \"Profitability\": \"#d62728\"}\n",
    "for name, color in colors_ind.items():\n",
    "    cum = (1 + results[name].portfolio_returns).cumprod()\n",
    "    cum.plot(ax=ax1, label=f\"{name} (SR={summaries[name]['sharpe_ratio']:.2f})\",\n",
    "             color=color, linewidth=1, alpha=0.8)\n",
    "ax1.axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.8)\n",
    "ax1.set_ylabel(\"Cumulative Return\")\n",
    "ax1.set_title(\"Individual Factor Strategies\")\n",
    "ax1.legend(loc=\"upper left\")\n",
    "\n",
    "colors_comp = {\"EqualWeight\": \"#1f77b4\", \"ICWeighted\": \"#ff7f0e\", \"RankCombo\": \"#2ca02c\"}\n",
    "for name, color in colors_comp.items():\n",
    "    cum = (1 + results[name].portfolio_returns).cumprod()\n",
    "    cum.plot(ax=ax2, label=f\"{name} (SR={summaries[name]['sharpe_ratio']:.2f})\",\n",
    "             color=color, linewidth=1.5)\n",
    "ax2.axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.8)\n",
    "ax2.set_ylabel(\"Cumulative Return\")\n",
    "ax2.set_title(\"Composite Strategies - Equity Curves\")\n",
    "ax2.legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbbec77",
   "metadata": {},
   "source": [
    "## 6. Marginal contribution — leave-one-out analysis\n",
    "\n",
    "To assess each signal's **marginal contribution**, we build the equal-weight\n",
    "composite *without* that signal and compare the Sharpe ratio to the full\n",
    "composite.\n",
    "\n",
    "- **Positive delta**: removing the signal *hurts* the composite (the signal adds value).\n",
    "- **Negative delta**: removing the signal *helps* (the signal is a drag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ecee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sharpe = summaries[\"EqualWeight\"][\"sharpe_ratio\"]\n",
    "loo_sharpes = {}\n",
    "sharpe_deltas = {}\n",
    "\n",
    "for drop_name in signal_names:\n",
    "    remaining = [n for n in signal_names if n != drop_name]\n",
    "    loo_sig = sum(signals[n] for n in remaining) / len(remaining)\n",
    "    loo_sig = zscore(loo_sig.dropna())\n",
    "\n",
    "    w = equal_weight_long_short(loo_sig, long_pct=0.2, short_pct=0.2)\n",
    "    w = normalize_weights(w, gross_exposure=2.0, net_exposure=0.0)\n",
    "    w = apply_position_limits(w, max_weight=0.10, min_weight=-0.10)\n",
    "    res = run_backtest(w, prices, config=config)\n",
    "    loo_sr = performance_summary(res.portfolio_returns)[\"sharpe_ratio\"]\n",
    "    loo_sharpes[drop_name] = loo_sr\n",
    "    sharpe_deltas[drop_name] = full_sharpe - loo_sr\n",
    "    print(f\"  Drop {drop_name:15s}: LOO Sharpe={loo_sr:+.3f}  \"\n",
    "          f\"Delta={sharpe_deltas[drop_name]:+.3f}\")\n",
    "\n",
    "print(f\"\\nFull EqualWeight composite Sharpe: {full_sharpe:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd83fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of Sharpe deltas\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "names = list(sharpe_deltas.keys())\n",
    "deltas = list(sharpe_deltas.values())\n",
    "colors = [\"#388e3c\" if d > 0 else \"#d32f2f\" for d in deltas]\n",
    "bars = ax.bar(names, deltas, color=colors, edgecolor=\"black\", linewidth=0.5)\n",
    "\n",
    "for bar, delta in zip(bars, deltas):\n",
    "    y = bar.get_height()\n",
    "    offset = 0.005 if abs(y) > 0.01 else 0.002\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, y + offset * np.sign(y),\n",
    "            f\"{delta:+.3f}\", ha=\"center\", va=\"bottom\" if y >= 0 else \"top\",\n",
    "            fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "ax.axhline(0, color=\"black\", linewidth=0.8)\n",
    "ax.set_ylabel(\"Sharpe Delta (Full Composite - Leave-One-Out)\")\n",
    "ax.set_title(\"Marginal Contribution: Sharpe Impact of Each Signal\")\n",
    "ax.set_xlabel(\"Dropped Signal\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Green bars (positive delta) = signal adds value to the composite.\")\n",
    "print(\"Red bars (negative delta) = signal detracts from the composite.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417a70d",
   "metadata": {},
   "source": [
    "## 7. Correlation stability over time\n",
    "\n",
    "Signal correlations are not static. We compute the **rolling 126-day\n",
    "cross-sectional Spearman correlation** between the momentum and reversal\n",
    "signals. Instability here is a risk for combination strategies that assume\n",
    "fixed signal relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06695a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling 126-day cross-sectional correlation: Momentum vs Reversal\n",
    "roll_window = 126\n",
    "rolling_corrs = []\n",
    "rolling_corr_dates = []\n",
    "\n",
    "for d_idx in range(roll_window, len(dates)):\n",
    "    d = dates[d_idx]\n",
    "    window_dates = dates[d_idx - roll_window:d_idx]\n",
    "    corrs_window = []\n",
    "    for wd in window_dates[::3]:  # subsample for speed\n",
    "        try:\n",
    "            s_m = signals[\"Momentum\"].loc[wd].dropna()\n",
    "            s_r = signals[\"Reversal\"].loc[wd].dropna()\n",
    "            common_t = s_m.index.intersection(s_r.index)\n",
    "            if len(common_t) >= 5:\n",
    "                rho, _ = spearmanr(s_m.loc[common_t].values,\n",
    "                                   s_r.loc[common_t].values)\n",
    "                if not np.isnan(rho):\n",
    "                    corrs_window.append(rho)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if corrs_window:\n",
    "        rolling_corrs.append(np.mean(corrs_window))\n",
    "        rolling_corr_dates.append(d)\n",
    "\n",
    "rolling_corr_series = pd.Series(rolling_corrs, index=rolling_corr_dates)\n",
    "print(f\"Rolling 126-day correlation (Momentum vs Reversal):\")\n",
    "print(f\"  Mean:   {rolling_corr_series.mean():.3f}\")\n",
    "print(f\"  Std:    {rolling_corr_series.std():.3f}\")\n",
    "print(f\"  Min:    {rolling_corr_series.min():.3f}\")\n",
    "print(f\"  Max:    {rolling_corr_series.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.plot(rolling_corr_series.index, rolling_corr_series.values,\n",
    "        color=\"navy\", linewidth=1.2, label=\"Momentum vs Reversal\")\n",
    "ax.axhline(rolling_corr_series.mean(), color=\"crimson\", linestyle=\"--\",\n",
    "           linewidth=1, label=f\"Mean ({rolling_corr_series.mean():.2f})\")\n",
    "ax.fill_between(rolling_corr_series.index,\n",
    "                rolling_corr_series.mean() - rolling_corr_series.std(),\n",
    "                rolling_corr_series.mean() + rolling_corr_series.std(),\n",
    "                alpha=0.15, color=\"crimson\", label=\"+/- 1 std band\")\n",
    "ax.axhline(0, color=\"grey\", linewidth=0.5)\n",
    "ax.set_ylabel(\"Spearman Correlation\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_title(\"Rolling 126-Day Cross-Sectional Correlation: Momentum vs Reversal\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStable correlation = reliable diversification benefit.\")\n",
    "print(\"Wide swings = combination may fail when you need it most.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853ba83",
   "metadata": {},
   "source": [
    "## 8. What can go wrong\n",
    "\n",
    "### 8.1 Overfitting combination weights\n",
    "\n",
    "Optimising signal weights on historical data is tempting but dangerous.\n",
    "IC estimates are noisy; more complex optimisation (e.g. mean-variance on\n",
    "signal returns) compounds estimation error. **Shrinkage toward equal weight**\n",
    "is essential — we used 50% above, and even that may be too aggressive.\n",
    "\n",
    "**Mitigations**: walk-forward estimation, heavy shrinkage, prefer rank-based\n",
    "combination over z-score-based.\n",
    "\n",
    "### 8.2 Common factor exposures\n",
    "\n",
    "If multiple signals load on the same underlying factor (market beta, size,\n",
    "sector), combining them does not truly diversify. The composite amplifies\n",
    "hidden, unintended bets.\n",
    "\n",
    "**Diagnostic**: regress composite returns on standard factor models. A small\n",
    "alpha relative to large factor loadings means the composite is a factor bet\n",
    "in disguise.\n",
    "\n",
    "### 8.3 Correlation breakdown in stress\n",
    "\n",
    "During market crises, cross-sectional correlations spike: all stocks move\n",
    "together, all alpha signals lose dispersion simultaneously. The\n",
    "diversification benefit of combining signals vanishes precisely when it\n",
    "is needed most.\n",
    "\n",
    "### 8.4 Signal interaction effects\n",
    "\n",
    "Signals can interact non-linearly. A high-momentum, high-volatility stock\n",
    "behaves very differently from a high-momentum, low-volatility stock. Simple\n",
    "linear combinations miss these interactions. Conditional combination or\n",
    "interaction terms can help but add overfitting risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14537a60",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "print(\"=\" * 95)\n",
    "print(\"MULTI-FACTOR COMPOSITION - FINAL SUMMARY\")\n",
    "print(\"=\" * 95)\n",
    "print(f\"\\nUniverse: {len(TICKERS)} US equities, {START} to {END}\")\n",
    "print(f\"Signals: {', '.join(signal_names)}\")\n",
    "print(f\"Average pairwise signal correlation: \"\n",
    "      f\"{(corr_matrix.sum() - n_signals) / (n_signals * (n_signals - 1)):.3f}\")\n",
    "print()\n",
    "\n",
    "cols = [\"Ann. Return\", \"Ann. Vol\", \"Sharpe\", \"Max DD\", \"Type\"]\n",
    "print(summary_df[cols].to_string())\n",
    "print()\n",
    "\n",
    "# Best composite vs best individual\n",
    "best_composite = summary_df.loc[summary_df[\"Type\"] == \"Composite\", \"Sharpe\"].idxmax()\n",
    "best_comp_sr = summary_df.loc[best_composite, \"Sharpe\"]\n",
    "best_individual = summary_df.loc[summary_df[\"Type\"] == \"Individual\", \"Sharpe\"].idxmax()\n",
    "best_ind_sr = summary_df.loc[best_individual, \"Sharpe\"]\n",
    "print(f\"Best composite:  {best_composite} (Sharpe = {best_comp_sr:.3f})\")\n",
    "print(f\"Best individual: {best_individual} (Sharpe = {best_ind_sr:.3f})\")\n",
    "print(f\"Improvement:     {best_comp_sr - best_ind_sr:+.3f} Sharpe units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ad227",
   "metadata": {},
   "source": [
    "### Key takeaways\n",
    "\n",
    "| Concept | Takeaway |\n",
    "|---------|----------|\n",
    "| **Signal diversity** | Combine signals with low cross-sectional correlation |\n",
    "| **Equal weight** | Hard to beat; robust baseline with no look-ahead bias |\n",
    "| **IC weighting** | Modest improvement possible, but shrinkage is essential |\n",
    "| **Rank-then-combine** | Robust to outliers; nonparametric and stable |\n",
    "| **Marginal contribution** | Leave-one-out reveals which signals earn their place |\n",
    "| **Correlation stability** | Signal relationships drift over time — monitor them |\n",
    "| **Overfitting** | The fancier the combination, the higher the overfitting risk |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
