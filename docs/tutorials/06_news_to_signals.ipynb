{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4afbd89f",
   "metadata": {},
   "source": [
    "# Tutorial 6 — From Market News to Quantitative Signals\n",
    "\n",
    "## Overview\n",
    "\n",
    "News drives short-term price discovery.  Earnings surprises, analyst upgrades,\n",
    "product launches, and regulatory actions all generate **discrete information\n",
    "shocks** that can be mapped to trading signals.\n",
    "\n",
    "This tutorial shows how to:\n",
    "1. Generate and structure synthetic news/event data.\n",
    "2. Aggregate event-level data into daily cross-sectional signals.\n",
    "3. Conduct an **event study** to measure the price impact of news.\n",
    "4. Apply **exponential decay** to model fading information content.\n",
    "5. Build and backtest three news-based strategies (momentum, reversal,\n",
    "   volatility avoidance).\n",
    "6. Combine news signals with price-based momentum.\n",
    "7. Understand backtesting pitfalls specific to event-driven strategies.\n",
    "\n",
    "> **Data note**: Price data is real (via yfinance).  News data is entirely\n",
    "> *synthetic* — generated programmatically — because no free, redistributable\n",
    "> historical news corpus exists for the tickers and date range we use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1008a7",
   "metadata": {},
   "source": [
    "## 1. Setup and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b680e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 5), \"figure.dpi\": 100,\n",
    "                     \"axes.grid\": True, \"grid.alpha\": 0.3})\n",
    "\n",
    "from qlab.data import YFinanceProvider, ParquetCache\n",
    "from qlab.features import simple_returns, realized_volatility, rank, zscore\n",
    "from qlab.features.rolling import rolling_mean, rolling_std\n",
    "from qlab.portfolio import equal_weight_long_short, normalize_weights\n",
    "from qlab.backtest import run_backtest, BacktestConfig\n",
    "from qlab.risk import performance_summary, drawdown_series, drawdown_details\n",
    "\n",
    "TICKERS = [\n",
    "    \"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"META\",\n",
    "    \"JPM\", \"GS\", \"BAC\",\n",
    "    \"JNJ\", \"PFE\", \"UNH\",\n",
    "    \"XOM\", \"CVX\",\n",
    "    \"PG\", \"KO\", \"WMT\",\n",
    "    \"HD\", \"NKE\",\n",
    "    \"CAT\", \"HON\",\n",
    "]\n",
    "START, END = \"2018-01-01\", \"2024-12-31\"\n",
    "\n",
    "provider = ParquetCache(YFinanceProvider(), cache_dir=\".qlab_cache\")\n",
    "prices = provider.fetch(TICKERS, START, END)\n",
    "close = prices[\"adj_close\"]\n",
    "ret = simple_returns(close)\n",
    "\n",
    "trading_dates = close.index.get_level_values(\"date\").unique().sort_values()\n",
    "print(f\"Universe : {close.index.get_level_values('ticker').nunique()} stocks\")\n",
    "print(f\"Date range: {trading_dates.min().date()} to {trading_dates.max().date()}\")\n",
    "print(f\"Total obs : {len(close):,}\")\n",
    "print(f\"Trading days: {len(trading_dates):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4650beb0",
   "metadata": {},
   "source": [
    "## 2. Generate synthetic news data\n",
    "\n",
    "We create a realistic synthetic news dataset with the following properties:\n",
    "\n",
    "- **~30 events per stock per year** (~4,200 total per year for the 20-stock universe).\n",
    "- Events **cluster around quarterly earnings months** (Jan, Apr, Jul, Oct) to mimic real\n",
    "  news flow.\n",
    "- **Sentiment** ranges from -1 (very negative) to +1 (very positive) and follows\n",
    "  a slightly positive-skewed distribution (reflecting the well-documented positivity\n",
    "  bias in financial news).\n",
    "- **Magnitude** in [0, 1] captures the importance of the event.\n",
    "- **Category** labels: `earnings`, `analyst`, `product`, `regulatory`, `macro`, `other`.\n",
    "- **Source** labels: `reuters`, `bloomberg`, `wsj`, `cnbc`, `other`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d0f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_news(tickers, start, end, rng=None):\n",
    "    \"\"\"Generate a synthetic news DataFrame with realistic clustering properties.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tickers : list[str]\n",
    "    start, end : str  (YYYY-MM-DD)\n",
    "    rng : np.random.Generator, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with columns: timestamp, ticker, sentiment, magnitude, category, source\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(42)\n",
    "\n",
    "    start_dt = pd.Timestamp(start)\n",
    "    end_dt = pd.Timestamp(end)\n",
    "    n_years = (end_dt - start_dt).days / 365.25\n",
    "    events_per_stock_per_year = 30\n",
    "    n_events_per_stock = int(events_per_stock_per_year * n_years)\n",
    "\n",
    "    categories = [\"earnings\", \"analyst\", \"product\", \"regulatory\", \"macro\", \"other\"]\n",
    "    cat_weights = np.array([0.20, 0.25, 0.15, 0.10, 0.15, 0.15])\n",
    "    sources = [\"reuters\", \"bloomberg\", \"wsj\", \"cnbc\", \"other\"]\n",
    "    src_weights = np.array([0.25, 0.25, 0.20, 0.15, 0.15])\n",
    "\n",
    "    # Quarterly earnings months: Jan, Apr, Jul, Oct\n",
    "    earnings_months = {1, 4, 7, 10}\n",
    "\n",
    "    all_rows = []\n",
    "    total_days = (end_dt - start_dt).days\n",
    "\n",
    "    for ticker in tickers:\n",
    "        n_events = n_events_per_stock + rng.integers(-5, 6)  # slight variation\n",
    "\n",
    "        # Generate event timestamps — cluster 40% around earnings months\n",
    "        n_earnings_cluster = int(0.4 * n_events)\n",
    "        n_uniform = n_events - n_earnings_cluster\n",
    "\n",
    "        # Uniform spread events\n",
    "        uniform_offsets = rng.integers(0, total_days, size=n_uniform)\n",
    "        uniform_dates = [start_dt + pd.Timedelta(days=int(d)) for d in uniform_offsets]\n",
    "\n",
    "        # Earnings-cluster events: pick dates near mid-month of earnings months\n",
    "        earnings_dates_list = []\n",
    "        years_range = list(range(start_dt.year, end_dt.year + 1))\n",
    "        for _ in range(n_earnings_cluster):\n",
    "            yr = rng.choice(years_range)\n",
    "            mo = rng.choice(list(earnings_months))\n",
    "            # Day clustered around 15th +/- 10 days\n",
    "            day_offset = int(np.clip(rng.normal(15, 5), 1, 28))\n",
    "            dt = pd.Timestamp(year=yr, month=mo, day=day_offset)\n",
    "            if start_dt <= dt <= end_dt:\n",
    "                earnings_dates_list.append(dt)\n",
    "\n",
    "        all_dates = uniform_dates + earnings_dates_list\n",
    "\n",
    "        for dt in all_dates:\n",
    "            # Add random hour (news can arrive anytime)\n",
    "            hour = rng.choice([7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n",
    "            minute = rng.integers(0, 60)\n",
    "            ts = dt.replace(hour=hour, minute=int(minute))\n",
    "\n",
    "            # Sentiment: slight positive bias, heavy tails\n",
    "            raw = rng.normal(0.05, 0.4)\n",
    "            sentiment = float(np.clip(raw, -1.0, 1.0))\n",
    "\n",
    "            # Magnitude: beta distribution skewed toward lower values\n",
    "            magnitude = float(rng.beta(2, 5))\n",
    "\n",
    "            # Category\n",
    "            cat = rng.choice(categories, p=cat_weights)\n",
    "            # Earnings events around earnings months are more likely \"earnings\"\n",
    "            if dt.month in earnings_months and rng.random() < 0.5:\n",
    "                cat = \"earnings\"\n",
    "\n",
    "            src = rng.choice(sources, p=src_weights)\n",
    "\n",
    "            all_rows.append({\n",
    "                \"timestamp\": ts,\n",
    "                \"ticker\": ticker,\n",
    "                \"sentiment\": round(sentiment, 4),\n",
    "                \"magnitude\": round(magnitude, 4),\n",
    "                \"category\": cat,\n",
    "                \"source\": src,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "news = generate_synthetic_news(TICKERS, START, END, rng=rng)\n",
    "print(f\"Total synthetic news events: {len(news):,}\")\n",
    "print(f\"Date range: {news['timestamp'].min().date()} to {news['timestamp'].max().date()}\")\n",
    "print(f\"\\nEvents per ticker:\")\n",
    "print(news.groupby(\"ticker\").size().describe().to_string())\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(news[\"category\"].value_counts().to_string())\n",
    "print(f\"\\nSource distribution:\")\n",
    "print(news[\"source\"].value_counts().to_string())\n",
    "print(f\"\\nSentiment stats: mean={news['sentiment'].mean():.3f}, \"\n",
    "      f\"std={news['sentiment'].std():.3f}, \"\n",
    "      f\"skew={news['sentiment'].skew():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fd7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot event frequency over time and sentiment distribution\n",
    "news_plot = news.copy()\n",
    "news_plot[\"month\"] = news_plot[\"timestamp\"].dt.to_period(\"M\")\n",
    "monthly_counts = news_plot.groupby(\"month\").size()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Monthly event count\n",
    "ax = axes[0]\n",
    "monthly_counts.plot(kind=\"bar\", ax=ax, color=\"steelblue\", width=0.8)\n",
    "ax.set_title(\"Monthly News Event Count\")\n",
    "ax.set_ylabel(\"Number of events\")\n",
    "# Show every 6th label to avoid crowding\n",
    "tick_labels = [str(p) if i % 6 == 0 else \"\" for i, p in enumerate(monthly_counts.index)]\n",
    "ax.set_xticklabels(tick_labels, rotation=45, ha=\"right\")\n",
    "\n",
    "# Sentiment histogram\n",
    "ax2 = axes[1]\n",
    "ax2.hist(news[\"sentiment\"], bins=50, color=\"steelblue\", edgecolor=\"white\", alpha=0.8)\n",
    "ax2.axvline(0, color=\"red\", linestyle=\"--\", linewidth=1, label=\"Neutral\")\n",
    "ax2.axvline(news[\"sentiment\"].mean(), color=\"green\", linestyle=\"--\", linewidth=1,\n",
    "            label=f\"Mean={news['sentiment'].mean():.3f}\")\n",
    "ax2.set_xlabel(\"Sentiment\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Sentiment Distribution\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845b0052",
   "metadata": {},
   "source": [
    "## 3. Daily aggregation\n",
    "\n",
    "Raw news events arrive at arbitrary timestamps.  To build a tradeable signal we must\n",
    "map them to **trading days** with a clear rule for handling after-hours news:\n",
    "\n",
    "| Event time | Attribution |\n",
    "|-----------|-------------|\n",
    "| Before market open (< 09:30) | Same trading day |\n",
    "| During market hours | Same trading day |\n",
    "| After market close (>= 16:00) | **Next** trading day |\n",
    "| Weekend / holiday | Next trading day |\n",
    "\n",
    "The daily signal for each ticker is the sum of `sentiment x magnitude` for all\n",
    "events attributed to that day.  Days with no events receive a value of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf156acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_news_daily(news_df, trading_dates):\n",
    "    \"\"\"Aggregate news events to daily (date, ticker) signals.\n",
    "\n",
    "    After-hours events (>= 16:00) are attributed to the next trading day.\n",
    "    Weekend/holiday events are attributed to the next trading day.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    news_df : DataFrame\n",
    "        Must have columns: timestamp, ticker, sentiment, magnitude.\n",
    "    trading_dates : DatetimeIndex\n",
    "        Sorted trading calendar dates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    daily_agg : DataFrame\n",
    "        Columns: date, ticker, impact, n_events, avg_sentiment, avg_magnitude\n",
    "    \"\"\"\n",
    "    df = news_df.copy()\n",
    "\n",
    "    # Determine raw date and whether event is after-hours\n",
    "    df[\"raw_date\"] = df[\"timestamp\"].dt.normalize()\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "    # After-hours events (>= 16:00) get pushed to next calendar day\n",
    "    after_hours_mask = df[\"hour\"] >= 16\n",
    "    df.loc[after_hours_mask, \"raw_date\"] = df.loc[after_hours_mask, \"raw_date\"] + pd.Timedelta(days=1)\n",
    "\n",
    "    # Map each raw_date to the next available trading day\n",
    "    td_arr = trading_dates.values\n",
    "\n",
    "    def next_trading_day(dt):\n",
    "        mask = td_arr >= dt\n",
    "        if mask.any():\n",
    "            return pd.Timestamp(td_arr[mask][0])\n",
    "        return pd.NaT\n",
    "\n",
    "    unique_raw = df[\"raw_date\"].unique()\n",
    "    date_map = {d: next_trading_day(d) for d in unique_raw}\n",
    "    df[\"trade_date\"] = df[\"raw_date\"].map(date_map)\n",
    "    df = df.dropna(subset=[\"trade_date\"])\n",
    "\n",
    "    # Compute impact = sentiment x magnitude\n",
    "    df[\"impact\"] = df[\"sentiment\"] * df[\"magnitude\"]\n",
    "\n",
    "    # Aggregate per (trade_date, ticker)\n",
    "    agg = df.groupby([\"trade_date\", \"ticker\"]).agg(\n",
    "        impact=(\"impact\", \"sum\"),\n",
    "        n_events=(\"impact\", \"size\"),\n",
    "        avg_sentiment=(\"sentiment\", \"mean\"),\n",
    "        avg_magnitude=(\"magnitude\", \"mean\"),\n",
    "    ).reset_index()\n",
    "    agg.rename(columns={\"trade_date\": \"date\"}, inplace=True)\n",
    "\n",
    "    return agg\n",
    "\n",
    "daily_news = aggregate_news_daily(news, trading_dates)\n",
    "print(f\"Daily aggregated rows: {len(daily_news):,}\")\n",
    "print(f\"Unique dates with news: {daily_news['date'].nunique()}\")\n",
    "print(f\"Unique tickers with news: {daily_news['ticker'].nunique()}\")\n",
    "\n",
    "# Coverage: what fraction of (date, ticker) pairs have news?\n",
    "total_possible = len(trading_dates) * len(TICKERS)\n",
    "coverage = len(daily_news) / total_possible\n",
    "print(f\"\\nCoverage: {len(daily_news):,} / {total_possible:,} = {coverage:.1%}\")\n",
    "print(f\"Average events per day-ticker (when present): {daily_news['n_events'].mean():.2f}\")\n",
    "print(f\"\\nImpact statistics:\")\n",
    "print(daily_news[\"impact\"].describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a87060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert daily_news to a stacked Series with MultiIndex (date, ticker)\n",
    "daily_impact = daily_news.set_index([\"date\", \"ticker\"])[\"impact\"]\n",
    "daily_impact = daily_impact.sort_index()\n",
    "\n",
    "# Build a full panel: fill missing (date, ticker) with 0\n",
    "full_idx = pd.MultiIndex.from_product(\n",
    "    [trading_dates, TICKERS], names=[\"date\", \"ticker\"]\n",
    ")\n",
    "news_signal_raw = daily_impact.reindex(full_idx).fillna(0.0)\n",
    "news_signal_raw.name = \"news_impact\"\n",
    "\n",
    "print(f\"Full panel shape: {len(news_signal_raw):,} observations\")\n",
    "print(f\"Non-zero fraction: {(news_signal_raw != 0).mean():.3%}\")\n",
    "print(f\"\\nDaily cross-sectional stats (of the raw impact):\")\n",
    "daily_cs = news_signal_raw.groupby(level=\"date\").agg([\"mean\", \"std\", \"min\", \"max\"])\n",
    "print(daily_cs.describe().round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a31965",
   "metadata": {},
   "source": [
    "## 4. Event study\n",
    "\n",
    "An **event study** measures average abnormal returns around news events.  We define:\n",
    "\n",
    "$$\\text{Abnormal Return}_t = r_{i,t} - \\bar{r}_{\\text{mkt},t}$$\n",
    "\n",
    "where the market return is the equal-weight average across all tickers.\n",
    "\n",
    "We examine a window of **[-5, +10]** days around each event, split by positive\n",
    "vs negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dcfe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_study(news_df, returns, trading_dates, pre=5, post=10, min_events=30):\n",
    "    \"\"\"Compute average abnormal returns around news events.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    news_df : DataFrame\n",
    "        Daily aggregated news with columns: date, ticker, avg_sentiment.\n",
    "    returns : Series\n",
    "        Daily returns with MultiIndex (date, ticker).\n",
    "    trading_dates : DatetimeIndex\n",
    "        Sorted trading calendar.\n",
    "    pre, post : int\n",
    "        Number of days before/after the event.\n",
    "    min_events : int\n",
    "        Minimum events per bucket to include.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : dict with keys 'positive' and 'negative', each a DataFrame\n",
    "             with columns: offset, mean_ar, std_ar, count, se\n",
    "    \"\"\"\n",
    "    # Compute market return (equal-weight average each day)\n",
    "    mkt_ret = returns.groupby(level=\"date\").mean()\n",
    "\n",
    "    # Abnormal returns\n",
    "    dates_idx = returns.index.get_level_values(\"date\")\n",
    "    abnormal = returns - mkt_ret.reindex(dates_idx).values\n",
    "\n",
    "    # Build a date-to-integer position map for fast offset lookup\n",
    "    date_to_pos = {d: i for i, d in enumerate(trading_dates)}\n",
    "    n_dates = len(trading_dates)\n",
    "\n",
    "    # Split events into positive and negative sentiment\n",
    "    pos_events = news_df[news_df[\"avg_sentiment\"] > 0][[\"date\", \"ticker\"]].values\n",
    "    neg_events = news_df[news_df[\"avg_sentiment\"] < 0][[\"date\", \"ticker\"]].values\n",
    "\n",
    "    offsets = list(range(-pre, post + 1))\n",
    "    result = {}\n",
    "\n",
    "    for label, events in [(\"positive\", pos_events), (\"negative\", neg_events)]:\n",
    "        ar_by_offset = {o: [] for o in offsets}\n",
    "\n",
    "        for event_date, ticker in events:\n",
    "            if event_date not in date_to_pos:\n",
    "                continue\n",
    "            event_pos = date_to_pos[event_date]\n",
    "\n",
    "            for offset in offsets:\n",
    "                target_pos = event_pos + offset\n",
    "                if 0 <= target_pos < n_dates:\n",
    "                    target_date = trading_dates[target_pos]\n",
    "                    key = (target_date, ticker)\n",
    "                    if key in abnormal.index:\n",
    "                        ar_by_offset[offset].append(abnormal.loc[key])\n",
    "\n",
    "        rows = []\n",
    "        for o in offsets:\n",
    "            vals = ar_by_offset[o]\n",
    "            if len(vals) >= min_events:\n",
    "                arr = np.array(vals)\n",
    "                rows.append({\n",
    "                    \"offset\": o,\n",
    "                    \"mean_ar\": arr.mean(),\n",
    "                    \"std_ar\": arr.std(),\n",
    "                    \"count\": len(arr),\n",
    "                    \"se\": arr.std() / np.sqrt(len(arr)),\n",
    "                })\n",
    "        result[label] = pd.DataFrame(rows)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Run event study\n",
    "event_result = event_study(daily_news, ret.dropna(), trading_dates, pre=5, post=10)\n",
    "\n",
    "for label in [\"positive\", \"negative\"]:\n",
    "    df_es = event_result[label]\n",
    "    print(f\"\\n{label.upper()} sentiment events:\")\n",
    "    print(f\"  Events per offset: {df_es['count'].mean():.0f} (avg)\")\n",
    "    day0 = df_es.loc[df_es['offset'] == 0, 'mean_ar'].values\n",
    "    if len(day0) > 0:\n",
    "        print(f\"  Day 0 mean abnormal return: {day0[0]*100:.4f}%\")\n",
    "    day1 = df_es.loc[df_es['offset'] == 1, 'mean_ar'].values\n",
    "    if len(day1) > 0:\n",
    "        print(f\"  Day +1 mean abnormal return: {day1[0]*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot event study results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for label, color, marker in [(\"positive\", \"#388e3c\", \"o\"), (\"negative\", \"#d32f2f\", \"s\")]:\n",
    "    df_es = event_result[label]\n",
    "    ax.plot(df_es[\"offset\"], df_es[\"mean_ar\"] * 100, color=color, marker=marker,\n",
    "            linewidth=2, markersize=5, label=f\"{label.capitalize()} sentiment\")\n",
    "    ax.fill_between(df_es[\"offset\"],\n",
    "                    (df_es[\"mean_ar\"] - 1.96 * df_es[\"se\"]) * 100,\n",
    "                    (df_es[\"mean_ar\"] + 1.96 * df_es[\"se\"]) * 100,\n",
    "                    alpha=0.15, color=color)\n",
    "\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.7, label=\"Event day\")\n",
    "ax.axhline(0, color=\"grey\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.set_xlabel(\"Event Day Offset\", fontsize=12)\n",
    "ax.set_ylabel(\"Average Abnormal Return (%)\", fontsize=12)\n",
    "ax.set_title(\"Event Study: Abnormal Returns Around News Events\", fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_xlim(-5.5, 10.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52118a01",
   "metadata": {},
   "source": [
    "## 5. Decay functions\n",
    "\n",
    "News impact does not last forever.  We model information decay with an\n",
    "**exponential kernel**:\n",
    "\n",
    "$$s_{i,t}^{\\text{decayed}} = \\sum_{k=0}^{K} \\exp\\!\\left(-\\frac{\\ln 2}{\\tau} \\cdot k\\right) \\cdot x_{i,t-k}$$\n",
    "\n",
    "where $\\tau$ is the half-life in days and $K$ is the maximum lag.\n",
    "\n",
    "This creates a smoother, more continuous signal from the sparse raw events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604edb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_decay(signal, halflife=5, max_lag=21):\n",
    "    \"\"\"Apply exponential decay to a sparse event signal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : Series\n",
    "        MultiIndex (date, ticker) signal — typically sparse (many zeros).\n",
    "    halflife : int\n",
    "        Number of days for the signal to decay to half its original value.\n",
    "    max_lag : int\n",
    "        Maximum number of past days to include in the decay sum.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Series\n",
    "        Decayed signal with the same index.\n",
    "    \"\"\"\n",
    "    decay_rate = np.log(2) / halflife\n",
    "    decay_weights = np.exp(-decay_rate * np.arange(max_lag + 1))\n",
    "\n",
    "    # Unstack to wide form for vectorised computation\n",
    "    wide = signal.unstack(\"ticker\")\n",
    "    result = pd.DataFrame(0.0, index=wide.index, columns=wide.columns)\n",
    "\n",
    "    for lag in range(max_lag + 1):\n",
    "        shifted = wide.shift(lag).fillna(0.0)\n",
    "        result += decay_weights[lag] * shifted\n",
    "\n",
    "    # Re-stack to (date, ticker) MultiIndex\n",
    "    stacked = result.stack()\n",
    "    stacked.index.names = [\"date\", \"ticker\"]\n",
    "    stacked.name = \"news_decayed\"\n",
    "    return stacked\n",
    "\n",
    "# Apply decay with halflife=5, max_lag=21\n",
    "news_decayed = apply_decay(news_signal_raw, halflife=5, max_lag=21)\n",
    "\n",
    "# Compare coverage\n",
    "raw_nonzero = (news_signal_raw != 0).mean()\n",
    "decayed_nonzero = (news_decayed.abs() > 1e-8).mean()\n",
    "print(f\"Raw signal non-zero fraction:     {raw_nonzero:.3%}\")\n",
    "print(f\"Decayed signal non-zero fraction:  {decayed_nonzero:.3%}\")\n",
    "print(f\"Coverage amplification:            {decayed_nonzero / max(raw_nonzero, 1e-10):.1f}x\")\n",
    "\n",
    "print(f\"\\nDecayed signal statistics:\")\n",
    "print(news_decayed.describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f28bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raw vs decayed signal for a sample ticker\n",
    "sample_ticker = \"AAPL\"\n",
    "raw_ts = news_signal_raw.xs(sample_ticker, level=\"ticker\")\n",
    "decayed_ts = news_decayed.xs(sample_ticker, level=\"ticker\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 7), sharex=True)\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.bar(raw_ts.index, raw_ts.values, width=1, color=\"steelblue\", alpha=0.7)\n",
    "ax1.set_ylabel(\"Raw Impact\")\n",
    "ax1.set_title(f\"Raw News Signal — {sample_ticker}\")\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(decayed_ts.index, decayed_ts.values, color=\"darkorange\", linewidth=1)\n",
    "ax2.fill_between(decayed_ts.index, decayed_ts.values, 0, alpha=0.2, color=\"darkorange\")\n",
    "ax2.set_ylabel(\"Decayed Signal\")\n",
    "ax2.set_title(f\"Exponentially Decayed Signal (halflife=5, max_lag=21) — {sample_ticker}\")\n",
    "ax2.set_xlabel(\"Date\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show decay kernel\n",
    "lags = np.arange(22)\n",
    "kernel = np.exp(-np.log(2) / 5 * lags)\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "ax.bar(lags, kernel, color=\"steelblue\", edgecolor=\"white\")\n",
    "ax.set_xlabel(\"Lag (trading days)\")\n",
    "ax.set_ylabel(\"Weight\")\n",
    "ax.set_title(\"Exponential Decay Kernel (halflife=5)\")\n",
    "ax.axhline(0.5, color=\"red\", linestyle=\"--\", linewidth=1, label=\"Half-life threshold\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f0d36",
   "metadata": {},
   "source": [
    "## 6. Three news-based strategies\n",
    "\n",
    "We build three strategies from the decayed news signal, each exploiting a\n",
    "different hypothesis about how news affects prices:\n",
    "\n",
    "| Strategy | Hypothesis | Signal |\n",
    "|----------|-----------|--------|\n",
    "| **News momentum** | Positive news leads to continued outperformance | `+decayed_signal` |\n",
    "| **News reversal** | Markets overreact to news | `-decayed_signal` |\n",
    "| **Volatility avoidance** | High news activity = high uncertainty = bad | `-abs(decayed_signal)` |\n",
    "\n",
    "All use **weekly rebalancing** with standard transaction costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4571443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-sectional normalisation of the decayed signal\n",
    "news_cs = zscore(news_decayed)\n",
    "\n",
    "# Helper function to build weights and run backtest\n",
    "def run_news_strategy(signal, label, rebalance_freq=\"weekly\"):\n",
    "    \"\"\"Build quintile long/short weights from signal and run backtest.\"\"\"\n",
    "    sig = signal.replace(0.0, np.nan).dropna()\n",
    "    # Only keep dates where we have enough non-zero signals for a meaningful sort\n",
    "    date_counts = sig.groupby(level=\"date\").size()\n",
    "    valid_dates = date_counts[date_counts >= 4].index\n",
    "    sig = sig.loc[sig.index.get_level_values(\"date\").isin(valid_dates)]\n",
    "\n",
    "    if len(sig) == 0:\n",
    "        print(f\"  [{label}] No valid signal dates!\")\n",
    "        return None, None\n",
    "\n",
    "    weights = equal_weight_long_short(sig, long_pct=0.2, short_pct=0.2)\n",
    "    weights = normalize_weights(weights, gross_exposure=2.0, net_exposure=0.0)\n",
    "\n",
    "    config = BacktestConfig(\n",
    "        rebalance_freq=rebalance_freq,\n",
    "        commission_bps=5.0,\n",
    "        slippage_bps=5.0,\n",
    "        signal_lag=1,\n",
    "        execution_price=\"open\",\n",
    "    )\n",
    "    result = run_backtest(weights, prices, config=config)\n",
    "    summary = performance_summary(result.portfolio_returns)\n",
    "\n",
    "    print(f\"  [{label}]\")\n",
    "    print(f\"    Sharpe:     {summary['sharpe_ratio']:.3f}\")\n",
    "    print(f\"    Ann. Ret:   {summary['annualized_return']:.3%}\")\n",
    "    print(f\"    Ann. Vol:   {summary['annualized_volatility']:.3%}\")\n",
    "    print(f\"    Max DD:     {summary['max_drawdown']:.3%}\")\n",
    "    print(f\"    Hit rate:   {summary['hit_rate']:.3%}\")\n",
    "    return result, summary\n",
    "\n",
    "print(\"Strategy backtests (weekly rebalance, 5bps cost each side):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Strategy 1: News Momentum (long positive sentiment, short negative)\n",
    "news_mom_sig = news_cs.copy()\n",
    "result_mom, summary_mom = run_news_strategy(news_mom_sig, \"News Momentum\")\n",
    "\n",
    "# Strategy 2: News Reversal (opposite sign)\n",
    "news_rev_sig = -news_cs.copy()\n",
    "result_rev, summary_rev = run_news_strategy(news_rev_sig, \"News Reversal\")\n",
    "\n",
    "# Strategy 3: Volatility Avoidance (negative of absolute magnitude activity)\n",
    "news_vol_sig = -news_cs.abs()\n",
    "news_vol_sig_z = zscore(news_vol_sig.replace(0.0, np.nan).dropna())\n",
    "result_vol, summary_vol = run_news_strategy(news_vol_sig_z, \"Vol Avoidance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc807f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual equity curves with drawdowns\n",
    "strategies = [\n",
    "    (\"News Momentum\", result_mom, summary_mom, \"navy\"),\n",
    "    (\"News Reversal\", result_rev, summary_rev, \"crimson\"),\n",
    "    (\"Vol Avoidance\", result_vol, summary_vol, \"darkorange\"),\n",
    "]\n",
    "\n",
    "for name, result, summary, color in strategies:\n",
    "    if result is None:\n",
    "        continue\n",
    "    cum = (1 + result.portfolio_returns).cumprod()\n",
    "    dd = drawdown_series(result.portfolio_returns)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True,\n",
    "                                    gridspec_kw={\"height_ratios\": [3, 1]})\n",
    "    cum.plot(ax=ax1, color=color, linewidth=1.5)\n",
    "    ax1.axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.8)\n",
    "    ax1.set_ylabel(\"Cumulative Return\")\n",
    "    ax1.set_title(f\"{name} — Equity Curve (SR={summary['sharpe_ratio']:.2f})\")\n",
    "\n",
    "    dd.plot(ax=ax2, color=color, linewidth=0.8)\n",
    "    ax2.fill_between(dd.index, dd.values, 0, alpha=0.3, color=color)\n",
    "    ax2.set_ylabel(\"Drawdown\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ced0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 3 equity curves on one chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for name, result, summary, color in strategies:\n",
    "    if result is None:\n",
    "        continue\n",
    "    cum = (1 + result.portfolio_returns).cumprod()\n",
    "    sr = summary[\"sharpe_ratio\"]\n",
    "    cum.plot(ax=ax, color=color, linewidth=1.5, label=f\"{name} (SR={sr:.2f})\")\n",
    "\n",
    "ax.axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.8)\n",
    "ax.set_ylabel(\"Cumulative Return\")\n",
    "ax.set_title(\"News-Based Strategies Comparison\")\n",
    "ax.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7889507",
   "metadata": {},
   "source": [
    "## 7. Combination with price momentum\n",
    "\n",
    "News signals can complement price-based signals.  We combine:\n",
    "- **News momentum** (our decayed sentiment signal)\n",
    "- **Price momentum** (12-month return, skip most recent month)\n",
    "\n",
    "The combined signal is a simple equal-weight average of the cross-sectionally\n",
    "normalised components.  If the two signals capture different information, the\n",
    "blend should have a **higher Sharpe ratio** and **lower drawdown** than either\n",
    "alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price momentum signal: 252-day return, skip 21 days\n",
    "# (12-minus-1 momentum: return over months 2-12)\n",
    "mom_252 = simple_returns(close, periods=252)\n",
    "mom_21 = simple_returns(close, periods=21)\n",
    "price_mom_raw = (1 + mom_252) / (1 + mom_21) - 1\n",
    "price_mom_cs = zscore(price_mom_raw.dropna())\n",
    "\n",
    "# Align the two signals on their common index\n",
    "common_idx = news_cs.dropna().index.intersection(price_mom_cs.dropna().index)\n",
    "print(f\"News signal observations:         {len(news_cs.dropna()):,}\")\n",
    "print(f\"Price momentum observations:      {len(price_mom_cs.dropna()):,}\")\n",
    "print(f\"Common index observations:        {len(common_idx):,}\")\n",
    "\n",
    "# Combined signal: equal-weight blend\n",
    "news_aligned = news_cs.reindex(common_idx).fillna(0)\n",
    "price_aligned = price_mom_cs.reindex(common_idx).fillna(0)\n",
    "combined = 0.5 * news_aligned + 0.5 * price_aligned\n",
    "combined = zscore(combined.dropna())\n",
    "\n",
    "# Backtest all three on the common index\n",
    "print(\"\\nIndividual and combined strategy comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# News only (on common dates)\n",
    "result_news_only, summary_news_only = run_news_strategy(\n",
    "    zscore(news_aligned), \"News Only\", rebalance_freq=\"weekly\"\n",
    ")\n",
    "\n",
    "# Price momentum only (on common dates)\n",
    "result_price_only, summary_price_only = run_news_strategy(\n",
    "    zscore(price_aligned), \"Price Mom Only\", rebalance_freq=\"weekly\"\n",
    ")\n",
    "\n",
    "# Combined\n",
    "result_combo, summary_combo = run_news_strategy(\n",
    "    combined, \"Combined (50/50)\", rebalance_freq=\"weekly\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ef15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot combined vs individual equity curves\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "curve_data = [\n",
    "    (\"News Momentum\", result_news_only, summary_news_only, \"navy\"),\n",
    "    (\"Price Momentum\", result_price_only, summary_price_only, \"seagreen\"),\n",
    "    (\"Combined (50/50)\", result_combo, summary_combo, \"darkorange\"),\n",
    "]\n",
    "\n",
    "for name, result, summary, color in curve_data:\n",
    "    if result is None:\n",
    "        continue\n",
    "    cum = (1 + result.portfolio_returns).cumprod()\n",
    "    sr = summary[\"sharpe_ratio\"]\n",
    "    lw = 2.0 if \"Combined\" in name else 1.3\n",
    "    ls = \"-\" if \"Combined\" in name else \"--\"\n",
    "    cum.plot(ax=ax, color=color, linewidth=lw, linestyle=ls,\n",
    "             label=f\"{name} (SR={sr:.2f})\")\n",
    "\n",
    "ax.axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.8)\n",
    "ax.set_ylabel(\"Cumulative Return\")\n",
    "ax.set_title(\"News + Price Momentum: Individual vs Combined\")\n",
    "ax.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-signal correlation\n",
    "sig_corr = news_aligned.groupby(level=\"date\").corr(price_aligned)\n",
    "print(f\"\\nAvg cross-sectional correlation (news vs price momentum): {sig_corr.mean():.3f}\")\n",
    "print(\"Low correlation => diversification benefit from combining.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce0917",
   "metadata": {},
   "source": [
    "## 8. Backtesting considerations for event-driven strategies\n",
    "\n",
    "Event-driven strategies introduce unique backtesting pitfalls beyond those\n",
    "in standard cross-sectional factor models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ebfaf",
   "metadata": {},
   "source": [
    "### 8.1 Lookahead bias\n",
    "\n",
    "The most dangerous error: using future information in signal construction.\n",
    "\n",
    "| Timing cutoff | Safe? | Why |\n",
    "|--------------|-------|-----|\n",
    "| Event at 14:00, trade at close same day | Possibly | Signal observed before execution |\n",
    "| Event at 17:00 (after-hours), trade at close same day | **NO** | Signal not yet available |\n",
    "| Event at 17:00, trade at next day open | Yes | Respects information flow |\n",
    "| Aggregated daily score using all events from day *t*, trade at close *t* | **NO** | After-hours events included |\n",
    "| Aggregated daily score using events through 15:59, trade at close *t* | Yes | Cutoff before execution |\n",
    "\n",
    "Our implementation attributes after-hours events to the next trading day,\n",
    "and applies a 1-day signal lag on top, providing a conservative buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2079c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookahead bias check: verify our timing\n",
    "news_timing = news.copy()\n",
    "news_timing[\"hour\"] = news_timing[\"timestamp\"].dt.hour\n",
    "\n",
    "pre_market = (news_timing[\"hour\"] < 10).sum()\n",
    "market_hours = ((news_timing[\"hour\"] >= 10) & (news_timing[\"hour\"] < 16)).sum()\n",
    "after_hours = (news_timing[\"hour\"] >= 16).sum()\n",
    "\n",
    "print(\"Timing distribution of news events:\")\n",
    "print(f\"  Pre-market (< 10:00):   {pre_market:>5,} ({pre_market/len(news):.1%})\")\n",
    "print(f\"  Market hours (10-15):   {market_hours:>5,} ({market_hours/len(news):.1%})\")\n",
    "print(f\"  After-hours (>= 16:00): {after_hours:>5,} ({after_hours/len(news):.1%})\")\n",
    "print(f\"\\nAfter-hours events are pushed to next trading day => no lookahead bias.\")\n",
    "print(f\"Signal lag = 1 day => additional buffer for stale data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24f65a",
   "metadata": {},
   "source": [
    "### 8.2 Event overlap\n",
    "\n",
    "When multiple events for the same ticker occur on the same day, aggregation\n",
    "can mask or amplify the true signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b63dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event overlap analysis\n",
    "events_per_day_ticker = daily_news.groupby([\"date\", \"ticker\"])[\"n_events\"].sum()\n",
    "multi_event = events_per_day_ticker[events_per_day_ticker > 1]\n",
    "single_event = events_per_day_ticker[events_per_day_ticker == 1]\n",
    "\n",
    "print(\"Event overlap statistics:\")\n",
    "print(f\"  Total (date, ticker) pairs with news:  {len(events_per_day_ticker):,}\")\n",
    "print(f\"  Single-event days:                     {len(single_event):,} ({len(single_event)/len(events_per_day_ticker):.1%})\")\n",
    "print(f\"  Multi-event days:                      {len(multi_event):,} ({len(multi_event)/len(events_per_day_ticker):.1%})\")\n",
    "print(f\"  Max events in one (date, ticker):      {events_per_day_ticker.max()}\")\n",
    "print(f\"\\nDistribution of events per (date, ticker):\")\n",
    "print(events_per_day_ticker.value_counts().sort_index().head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44edcbc",
   "metadata": {},
   "source": [
    "### 8.3 Sparse signals\n",
    "\n",
    "Unlike price-based factors (which have a value for every stock-day), news\n",
    "signals are **sparse**: most stock-days have zero events.  This creates challenges:\n",
    "\n",
    "- **Signal-to-noise ratio**: aggregating across many zero-signal days dilutes\n",
    "  the true information.\n",
    "- **Portfolio construction**: on days with only 2-3 non-zero signals, a quintile\n",
    "  sort is unstable.\n",
    "- **Turnover**: if events are rare but the rebalance is frequent, positions will\n",
    "  whipsaw between zero and non-zero.\n",
    "\n",
    "Our decay function (Section 5) partially addresses this by creating a continuous\n",
    "signal from discrete events.\n",
    "\n",
    "### 8.4 Delayed reaction\n",
    "\n",
    "Markets may not react immediately to news.  Some academic studies find that\n",
    "the full impact takes 1-3 days to materialise (especially for less-liquid\n",
    "stocks or complex information).  Our event study in Section 4 provides evidence\n",
    "on the speed of adjustment.\n",
    "\n",
    "### 8.5 Survivorship bias\n",
    "\n",
    "Our universe consists of 20 large-cap stocks that survived through the entire\n",
    "2018-2024 period.  This introduces **survivorship bias**: stocks that were\n",
    "delisted or declined significantly are excluded.  In practice, news-based\n",
    "strategies should use a **point-in-time universe** that includes all stocks\n",
    "that were tradeable on each historical date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de0bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity analysis: raw vs decayed signal coverage over time\n",
    "raw_nonzero_by_date = (news_signal_raw != 0).groupby(level=\"date\").sum()\n",
    "decayed_nonzero_by_date = (news_decayed.abs() > 1e-8).groupby(level=\"date\").sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(raw_nonzero_by_date.index, raw_nonzero_by_date.values,\n",
    "        alpha=0.5, linewidth=0.5, color=\"steelblue\", label=\"Raw (non-zero tickers/day)\")\n",
    "ax.plot(decayed_nonzero_by_date.index, decayed_nonzero_by_date.values,\n",
    "        alpha=0.7, linewidth=1, color=\"darkorange\", label=\"Decayed (non-zero tickers/day)\")\n",
    "ax.axhline(len(TICKERS), color=\"red\", linestyle=\"--\", linewidth=0.8,\n",
    "           label=f\"Full coverage ({len(TICKERS)})\")\n",
    "ax.set_ylabel(\"Tickers with signal\")\n",
    "ax.set_title(\"Signal Coverage: Raw vs Decayed\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0, len(TICKERS) + 2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average tickers with non-zero signal per day:\")\n",
    "print(f\"  Raw:     {raw_nonzero_by_date.mean():.1f} / {len(TICKERS)}\")\n",
    "print(f\"  Decayed: {decayed_nonzero_by_date.mean():.1f} / {len(TICKERS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de758c0",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16b3fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build summary table\n",
    "summary_data = []\n",
    "all_results = [\n",
    "    (\"News Momentum\", summary_mom),\n",
    "    (\"News Reversal\", summary_rev),\n",
    "    (\"Vol Avoidance\", summary_vol),\n",
    "    (\"Price Momentum\", summary_price_only),\n",
    "    (\"Combined (News+Price)\", summary_combo),\n",
    "]\n",
    "\n",
    "for name, s in all_results:\n",
    "    if s is not None:\n",
    "        summary_data.append({\n",
    "            \"Strategy\": name,\n",
    "            \"Ann. Return\": f\"{s['annualized_return']:.2%}\",\n",
    "            \"Ann. Vol\": f\"{s['annualized_volatility']:.2%}\",\n",
    "            \"Sharpe\": f\"{s['sharpe_ratio']:.3f}\",\n",
    "            \"Sortino\": f\"{s['sortino_ratio']:.3f}\",\n",
    "            \"Max DD\": f\"{s['max_drawdown']:.2%}\",\n",
    "            \"Calmar\": f\"{s['calmar_ratio']:.3f}\",\n",
    "            \"Hit Rate\": f\"{s['hit_rate']:.2%}\",\n",
    "        })\n",
    "\n",
    "summary_table = pd.DataFrame(summary_data)\n",
    "print(summary_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c407a7",
   "metadata": {},
   "source": [
    "## Key takeaways\n",
    "\n",
    "| Concept | Key takeaway |\n",
    "|---------|-------------|\n",
    "| News data | Discrete, irregular, sparse -- fundamentally different from price data |\n",
    "| Aggregation | After-hours attribution and daily summing are critical for avoiding lookahead |\n",
    "| Event study | Validates whether news has predictive content before building strategies |\n",
    "| Decay | Converts sparse events into continuous signals; half-life is a key parameter |\n",
    "| Strategies | Momentum, reversal, and volatility avoidance exploit different mechanisms |\n",
    "| Combination | Blending news with price signals can improve risk-adjusted returns |\n",
    "| Pitfalls | Lookahead bias, sparsity, event overlap, survivorship bias all require care |\n",
    "\n",
    "**Next**: Tutorial 7 explores multi-horizon signal blending and regime detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
