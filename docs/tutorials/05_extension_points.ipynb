{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3fdd24",
   "metadata": {},
   "source": [
    "# Tutorial 5 --- Extension Points for External Data\n",
    "\n",
    "## Overview\n",
    "\n",
    "Real-world quant research pipelines rarely rely on a single data source.\n",
    "You may pull prices from Yahoo Finance today, switch to a Bloomberg\n",
    "terminal tomorrow, and supplement with alternative data (news, sentiment,\n",
    "satellite imagery) next quarter.  **qlab** is designed for this:\n",
    "\n",
    "* A single abstract interface --- `MarketDataProvider` --- that every data\n",
    "  source implements.\n",
    "* A transparent `ParquetCache` decorator that makes any provider\n",
    "  reproducible and fast.\n",
    "* Clear data-format contracts so that every downstream function (features,\n",
    "  alphas, backtest) works without modification when you swap providers.\n",
    "\n",
    "This tutorial covers:\n",
    "\n",
    "1. The `MarketDataProvider` interface\n",
    "2. Live demonstration with `YFinanceProvider`\n",
    "3. Implementing a custom provider from scratch\n",
    "4. `ParquetCache` --- caching any provider to disk\n",
    "5. Designing a `NewsProvider` interface for alternative data\n",
    "6. Data format contracts\n",
    "7. Reproducibility checklist and recommended project layout\n",
    "8. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87088b",
   "metadata": {},
   "source": [
    "## 1. The `MarketDataProvider` Interface\n",
    "\n",
    "Every data source in qlab is a subclass of `MarketDataProvider`.  The\n",
    "contract is minimal --- implement **one** method:\n",
    "\n",
    "```\n",
    "fetch(tickers, start, end) -> DataFrame\n",
    "```\n",
    "\n",
    "The returned DataFrame must have:\n",
    "* A two-level `MultiIndex(date, ticker)`\n",
    "* Columns: `open, high, low, close, volume, adj_close`\n",
    "\n",
    "Let's inspect the actual source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd1a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from qlab.data.base import MarketDataProvider\n",
    "\n",
    "print(inspect.getsource(MarketDataProvider))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1e2b2",
   "metadata": {},
   "source": [
    "Key design decisions:\n",
    "\n",
    "| Decision | Rationale |\n",
    "|----------|-----------|\n",
    "| ABC with one abstract method | Minimal surface --- easy to implement |\n",
    "| Stacked MultiIndex output | Uniform shape regardless of ticker count |\n",
    "| `adj_close` included | Splits/dividends handled at the provider level |\n",
    "| `start` / `end` as strings or Timestamps | Flexible input, provider normalises internally |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff2ea01",
   "metadata": {},
   "source": [
    "## 2. Live Demonstration with `YFinanceProvider`\n",
    "\n",
    "`YFinanceProvider` wraps the popular `yfinance` package.  Let's fetch\n",
    "real market data for a small universe and validate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1dd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 5), \"figure.dpi\": 100,\n",
    "                     \"axes.grid\": True, \"grid.alpha\": 0.3})\n",
    "\n",
    "from qlab.data import YFinanceProvider, ParquetCache\n",
    "from qlab.utils.validation import validate_prices\n",
    "from qlab.features import simple_returns, rank\n",
    "from qlab.utils.alignment import unstack_to_wide\n",
    "\n",
    "# --- Fetch real data ---\n",
    "TICKERS = [\"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"META\"]\n",
    "START, END = \"2020-01-01\", \"2024-12-31\"\n",
    "\n",
    "provider = YFinanceProvider()\n",
    "prices = provider.fetch(TICKERS, START, END)\n",
    "\n",
    "# Validate using qlab's built-in checks\n",
    "validate_prices(prices)\n",
    "print(\"validate_prices passed!\")\n",
    "\n",
    "print(f\"\\nShape : {prices.shape}\")\n",
    "print(f\"Columns: {list(prices.columns)}\")\n",
    "print(f\"Index  : {prices.index.names}\")\n",
    "print(f\"Tickers: {sorted(prices.index.get_level_values('ticker').unique())}\")\n",
    "print(f\"Dates  : {prices.index.get_level_values('date').min().date()} to \"\n",
    "      f\"{prices.index.get_level_values('date').max().date()}\")\n",
    "print(f\"\\nHead:\")\n",
    "prices.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f17f7",
   "metadata": {},
   "source": [
    "### Price chart from live yfinance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstack to wide for plotting\n",
    "close_wide = unstack_to_wide(prices, column=\"adj_close\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "# Normalise to 100 at the start for comparability\n",
    "normalised = close_wide / close_wide.iloc[0] * 100\n",
    "for col in normalised.columns:\n",
    "    ax.plot(normalised.index, normalised[col], linewidth=1.5, label=col)\n",
    "ax.set_title(\"Normalised Adjusted Close (Base = 100)\", fontsize=14)\n",
    "ax.set_ylabel(\"Indexed Price\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quick stats\n",
    "daily_ret = simple_returns(prices[\"adj_close\"]).dropna()\n",
    "ann_ret = daily_ret.groupby(level=\"ticker\").mean() * 252\n",
    "ann_vol = daily_ret.groupby(level=\"ticker\").std() * np.sqrt(252)\n",
    "stats = pd.DataFrame({\"Ann Return\": ann_ret, \"Ann Vol\": ann_vol})\n",
    "stats[\"Sharpe\"] = stats[\"Ann Return\"] / stats[\"Ann Vol\"]\n",
    "print(stats.round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c62e2c",
   "metadata": {},
   "source": [
    "## 3. Implementing a Custom Provider\n",
    "\n",
    "Suppose your firm has a REST API that serves historical prices.  Here we\n",
    "build a `MockRestApiProvider` that generates realistic synthetic data\n",
    "using NumPy, following the exact same `MarketDataProvider` contract.\n",
    "\n",
    "The mock simulates geometric Brownian motion with per-ticker drift and\n",
    "volatility, plus realistic OHLCV structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlab.data.base import MarketDataProvider\n",
    "\n",
    "class MockRestApiProvider(MarketDataProvider):\n",
    "    \"\"\"Simulates a REST API returning daily OHLCV data.\n",
    "\n",
    "    Generates deterministic synthetic prices using geometric Brownian\n",
    "    motion so results are reproducible across runs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seed: int = 42):\n",
    "        self.seed = seed\n",
    "\n",
    "    def fetch(self, tickers, start, end):\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        start_ts = pd.Timestamp(start)\n",
    "        end_ts = pd.Timestamp(end)\n",
    "        dates = pd.bdate_range(start_ts, end_ts, name=\"date\")\n",
    "        n_days = len(dates)\n",
    "\n",
    "        parts = []\n",
    "        for ticker in tickers:\n",
    "            # Per-ticker parameters derived from ticker name for variety\n",
    "            ticker_hash = sum(ord(c) for c in ticker)\n",
    "            annual_drift = 0.05 + (ticker_hash % 20) * 0.01   # 5-25% drift\n",
    "            annual_vol = 0.15 + (ticker_hash % 15) * 0.01     # 15-30% vol\n",
    "            initial_price = 50 + (ticker_hash % 200)           # $50-$250\n",
    "\n",
    "            dt = 1 / 252\n",
    "            daily_drift = (annual_drift - 0.5 * annual_vol ** 2) * dt\n",
    "            daily_vol = annual_vol * np.sqrt(dt)\n",
    "\n",
    "            # GBM simulation\n",
    "            log_returns = rng.normal(daily_drift, daily_vol, n_days)\n",
    "            log_prices = np.log(initial_price) + np.cumsum(log_returns)\n",
    "            close_prices = np.exp(log_prices)\n",
    "\n",
    "            # Realistic OHLCV structure\n",
    "            intraday_range = rng.uniform(0.005, 0.025, n_days)\n",
    "            high = close_prices * (1 + intraday_range * rng.uniform(0.3, 1.0, n_days))\n",
    "            low = close_prices * (1 - intraday_range * rng.uniform(0.3, 1.0, n_days))\n",
    "            open_prices = low + (high - low) * rng.uniform(0.2, 0.8, n_days)\n",
    "            volume = rng.integers(500_000, 50_000_000, n_days).astype(float)\n",
    "\n",
    "            df = pd.DataFrame({\n",
    "                \"open\": open_prices,\n",
    "                \"high\": high,\n",
    "                \"low\": low,\n",
    "                \"close\": close_prices,\n",
    "                \"volume\": volume,\n",
    "                \"adj_close\": close_prices,  # no splits in synthetic data\n",
    "            }, index=dates)\n",
    "\n",
    "            df[\"ticker\"] = ticker\n",
    "            df = df.reset_index().set_index([\"date\", \"ticker\"])\n",
    "            parts.append(df)\n",
    "\n",
    "        return pd.concat(parts).sort_index()\n",
    "\n",
    "\n",
    "# --- Demonstrate the custom provider ---\n",
    "mock = MockRestApiProvider(seed=42)\n",
    "mock_tickers = [\"SYNTH_A\", \"SYNTH_B\", \"SYNTH_C\"]\n",
    "mock_prices = mock.fetch(mock_tickers, \"2022-01-01\", \"2024-06-30\")\n",
    "\n",
    "# Validate --- same function, no special handling needed\n",
    "validate_prices(mock_prices)\n",
    "print(\"MockRestApiProvider output passes validate_prices!\")\n",
    "print(f\"Shape   : {mock_prices.shape}\")\n",
    "print(f\"Tickers : {sorted(mock_prices.index.get_level_values('ticker').unique())}\")\n",
    "print(f\"Columns : {list(mock_prices.columns)}\")\n",
    "print(f\"\\nSample data:\")\n",
    "mock_prices.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e647a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that qlab features work seamlessly on mock data\n",
    "mock_returns = simple_returns(mock_prices[\"adj_close\"]).dropna()\n",
    "mock_ranked = rank(mock_returns)\n",
    "\n",
    "print(\"Daily returns (head):\")\n",
    "print(mock_returns.head(6).to_string())\n",
    "print(f\"\\nCross-sectional rank (head):\")\n",
    "print(mock_ranked.head(6).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec073786",
   "metadata": {},
   "source": [
    "## 4. `ParquetCache` --- Transparent Disk Caching\n",
    "\n",
    "`ParquetCache` wraps *any* `MarketDataProvider`.  On first call it\n",
    "fetches from the upstream provider and saves a Parquet file; subsequent\n",
    "calls read from disk.  This ensures:\n",
    "\n",
    "* **Reproducibility** --- you always use the same data snapshot.\n",
    "* **Speed** --- no network round-trip after the first fetch.\n",
    "* **Interoperability** --- Parquet files can be read by R, Julia, Spark, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, tempfile, shutil\n",
    "\n",
    "# Use a temporary directory so we don't pollute the workspace\n",
    "cache_dir = tempfile.mkdtemp(prefix=\"qlab_cache_demo_\")\n",
    "print(f\"Cache directory: {cache_dir}\")\n",
    "\n",
    "mock = MockRestApiProvider(seed=99)\n",
    "cached = ParquetCache(mock, cache_dir=cache_dir)\n",
    "\n",
    "# --- First fetch: cache MISS (data generated + written to disk) ---\n",
    "t0 = time.perf_counter()\n",
    "data1 = cached.fetch([\"ALPHA\", \"BETA\"], \"2023-01-01\", \"2023-12-31\")\n",
    "t_miss = time.perf_counter() - t0\n",
    "\n",
    "# --- Second fetch: cache HIT (read from Parquet) ---\n",
    "t0 = time.perf_counter()\n",
    "data2 = cached.fetch([\"ALPHA\", \"BETA\"], \"2023-01-01\", \"2023-12-31\")\n",
    "t_hit = time.perf_counter() - t0\n",
    "\n",
    "print(f\"Cache MISS (generate + write): {t_miss*1000:.1f} ms\")\n",
    "print(f\"Cache HIT  (read parquet):     {t_hit*1000:.1f} ms\")\n",
    "print(f\"Speed-up:                      {t_miss/max(t_hit, 1e-9):.1f}x\")\n",
    "print(f\"\\nData identical: {data1.equals(data2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c0caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Timing comparison bar chart ---\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "labels = [\"Cache MISS\\n(generate + write)\", \"Cache HIT\\n(read parquet)\"]\n",
    "times_ms = [t_miss * 1000, t_hit * 1000]\n",
    "colors = [\"#d32f2f\", \"#388e3c\"]\n",
    "bars = ax.bar(labels, times_ms, color=colors, width=0.5, edgecolor=\"black\", linewidth=0.8)\n",
    "\n",
    "for bar, val in zip(bars, times_ms):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + max(times_ms) * 0.02,\n",
    "            f\"{val:.1f} ms\", ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "ax.set_ylabel(\"Time (ms)\", fontsize=12)\n",
    "ax.set_title(\"ParquetCache: Miss vs Hit Latency\", fontsize=14)\n",
    "ax.set_ylim(0, max(times_ms) * 1.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up temporary cache\n",
    "shutil.rmtree(cache_dir)\n",
    "print(f\"Cleaned up {cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7029cb3",
   "metadata": {},
   "source": [
    "## 5. Designing a `NewsProvider` Interface\n",
    "\n",
    "Market data is just one type of input.  Let's design an alternative-data\n",
    "interface for **news events** and show how to convert unstructured events\n",
    "into a daily signal compatible with the qlab pipeline.\n",
    "\n",
    "### Design\n",
    "\n",
    "| Aspect | Price data | News data |\n",
    "|--------|-----------|-----------|\n",
    "| Frequency | Regular (one bar per trading day) | Irregular (zero or many events per day) |\n",
    "| Schema | Numeric OHLCV | Mixed (text, categories, scores) |\n",
    "| Coverage | Per-ticker, complete | Sparse (not every ticker every day) |\n",
    "\n",
    "We define:\n",
    "* `NewsEvent` --- a dataclass holding one event with sentiment.\n",
    "* `NewsProvider` --- an ABC with a `fetch_news` method.\n",
    "* `SyntheticNewsProvider` --- a concrete implementation generating mock events.\n",
    "* `aggregate_news_to_daily` --- converts events to a daily `(date, ticker)` signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4060dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class NewsEvent:\n",
    "    \"\"\"A single news event with sentiment.\"\"\"\n",
    "    timestamp: pd.Timestamp\n",
    "    ticker: str\n",
    "    headline: str\n",
    "    sentiment: float      # in [-1, 1]\n",
    "    source: str\n",
    "\n",
    "\n",
    "class NewsProvider(ABC):\n",
    "    \"\"\"Abstract base class for news data providers.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def fetch_news(\n",
    "        self,\n",
    "        tickers: list[str],\n",
    "        start: str,\n",
    "        end: str,\n",
    "    ) -> List[NewsEvent]:\n",
    "        \"\"\"Fetch news events for the given tickers and date range.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of NewsEvent\n",
    "            Sorted by timestamp.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class SyntheticNewsProvider(NewsProvider):\n",
    "    \"\"\"Generates deterministic mock news events for testing.\"\"\"\n",
    "\n",
    "    HEADLINES = [\n",
    "        \"{ticker} beats Q{q} earnings expectations\",\n",
    "        \"{ticker} announces new product launch\",\n",
    "        \"{ticker} faces regulatory scrutiny\",\n",
    "        \"{ticker} CEO steps down unexpectedly\",\n",
    "        \"{ticker} raises full-year guidance\",\n",
    "        \"{ticker} reports supply chain disruptions\",\n",
    "        \"{ticker} signs major partnership deal\",\n",
    "        \"{ticker} stock downgraded by analysts\",\n",
    "        \"{ticker} expands into new markets\",\n",
    "        \"{ticker} announces share buyback program\",\n",
    "        \"{ticker} revenue misses estimates\",\n",
    "        \"{ticker} wins government contract\",\n",
    "    ]\n",
    "    SOURCES = [\"Reuters\", \"Bloomberg\", \"CNBC\", \"WSJ\", \"MarketWatch\", \"FT\"]\n",
    "\n",
    "    def __init__(self, avg_events_per_ticker_per_month: float = 4.0, seed: int = 42):\n",
    "        self.avg_rate = avg_events_per_ticker_per_month\n",
    "        self.seed = seed\n",
    "\n",
    "    def fetch_news(self, tickers, start, end):\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        start_ts = pd.Timestamp(start)\n",
    "        end_ts = pd.Timestamp(end)\n",
    "        n_months = max(1, (end_ts - start_ts).days / 30)\n",
    "\n",
    "        events = []\n",
    "        for ticker in tickers:\n",
    "            n_events = rng.poisson(self.avg_rate * n_months)\n",
    "            # Random timestamps within the range\n",
    "            offsets = rng.uniform(0, (end_ts - start_ts).total_seconds(), n_events)\n",
    "            for offset in sorted(offsets):\n",
    "                ts = start_ts + pd.Timedelta(seconds=float(offset))\n",
    "                # Skip weekends\n",
    "                if ts.weekday() >= 5:\n",
    "                    continue\n",
    "                headline_template = rng.choice(self.HEADLINES)\n",
    "                headline = headline_template.format(\n",
    "                    ticker=ticker, q=rng.integers(1, 5)\n",
    "                )\n",
    "                # Sentiment: slightly positive bias with heavy tails\n",
    "                sentiment = float(np.clip(rng.normal(0.05, 0.4), -1, 1))\n",
    "                source = str(rng.choice(self.SOURCES))\n",
    "                events.append(NewsEvent(\n",
    "                    timestamp=ts,\n",
    "                    ticker=ticker,\n",
    "                    headline=headline,\n",
    "                    sentiment=sentiment,\n",
    "                    source=source,\n",
    "                ))\n",
    "\n",
    "        events.sort(key=lambda e: e.timestamp)\n",
    "        return events\n",
    "\n",
    "\n",
    "# --- Demonstrate ---\n",
    "news_provider = SyntheticNewsProvider(avg_events_per_ticker_per_month=5, seed=123)\n",
    "news_tickers = [\"AAPL\", \"MSFT\", \"GOOG\"]\n",
    "events = news_provider.fetch_news(news_tickers, \"2024-01-01\", \"2024-06-30\")\n",
    "\n",
    "print(f\"Total events generated: {len(events)}\")\n",
    "print(f\"\\nFirst 8 events:\")\n",
    "for e in events[:8]:\n",
    "    print(f\"  {e.timestamp.strftime('%Y-%m-%d %H:%M')}  {e.ticker:5s}  \"\n",
    "          f\"sent={e.sentiment:+.2f}  [{e.source}]  {e.headline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd2c44",
   "metadata": {},
   "source": [
    "### Aggregating events to a daily signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_news_to_daily(events: List[NewsEvent]) -> pd.Series:\n",
    "    \"\"\"Convert a list of NewsEvent into a daily (date, ticker) sentiment signal.\n",
    "\n",
    "    For each (date, ticker) pair, computes the mean sentiment across all\n",
    "    events on that day.  Days with no events for a ticker are left as NaN\n",
    "    (the caller can forward-fill or drop as needed).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    events : list of NewsEvent\n",
    "        Raw news events from any NewsProvider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        MultiIndex (date, ticker), values are mean daily sentiment.\n",
    "    \"\"\"\n",
    "    if not events:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    records = [\n",
    "        {\"date\": e.timestamp.normalize(), \"ticker\": e.ticker, \"sentiment\": e.sentiment}\n",
    "        for e in events\n",
    "    ]\n",
    "    df = pd.DataFrame(records)\n",
    "    daily = df.groupby([\"date\", \"ticker\"])[\"sentiment\"].mean()\n",
    "    daily.index = pd.MultiIndex.from_tuples(daily.index, names=[\"date\", \"ticker\"])\n",
    "    daily = daily.sort_index()\n",
    "    return daily\n",
    "\n",
    "\n",
    "# --- Aggregate and inspect ---\n",
    "news_signal = aggregate_news_to_daily(events)\n",
    "print(f\"Daily signal shape: {news_signal.shape[0]} observations\")\n",
    "print(f\"Date range: {news_signal.index.get_level_values('date').min().date()} to \"\n",
    "      f\"{news_signal.index.get_level_values('date').max().date()}\")\n",
    "print(f\"\\nMean sentiment by ticker:\")\n",
    "print(news_signal.groupby(level=\"ticker\").mean().round(3).to_string())\n",
    "print(f\"\\nSample values:\")\n",
    "print(news_signal.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10483b0",
   "metadata": {},
   "source": [
    "### News event timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1fb7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "for ax, ticker in zip(axes, news_tickers):\n",
    "    ticker_events = [e for e in events if e.ticker == ticker]\n",
    "    dates = [e.timestamp for e in ticker_events]\n",
    "    sentiments = [e.sentiment for e in ticker_events]\n",
    "\n",
    "    colors = [\"#388e3c\" if s >= 0 else \"#d32f2f\" for s in sentiments]\n",
    "    ax.bar(dates, sentiments, color=colors, width=1.5, alpha=0.7, edgecolor=\"none\")\n",
    "    ax.axhline(0, color=\"black\", linewidth=0.5)\n",
    "    ax.set_ylabel(\"Sentiment\")\n",
    "    ax.set_title(f\"{ticker} --- News Events\", fontsize=12)\n",
    "    ax.set_ylim(-1.1, 1.1)\n",
    "\n",
    "    # Overlay the daily aggregated signal\n",
    "    if ticker in news_signal.index.get_level_values(\"ticker\").unique():\n",
    "        ts = news_signal.xs(ticker, level=\"ticker\")\n",
    "        ax.plot(ts.index, ts.values, color=\"navy\", linewidth=1.5, alpha=0.8,\n",
    "                label=\"Daily mean\", marker=\"o\", markersize=3)\n",
    "        ax.legend(loc=\"upper right\", fontsize=9)\n",
    "\n",
    "axes[-1].set_xlabel(\"Date\")\n",
    "plt.suptitle(\"News Sentiment Timeline (Synthetic)\", fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Event count distribution\n",
    "event_df = pd.DataFrame([\n",
    "    {\"date\": e.timestamp.normalize(), \"ticker\": e.ticker} for e in events\n",
    "])\n",
    "daily_counts = event_df.groupby([\"date\", \"ticker\"]).size()\n",
    "print(f\"Events per (date, ticker) --- mean: {daily_counts.mean():.2f}, \"\n",
    "      f\"max: {daily_counts.max()}, min: {daily_counts.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1181073c",
   "metadata": {},
   "source": [
    "## 6. Data Format Contracts\n",
    "\n",
    "Every function in qlab expects data in specific formats.  Adhering to\n",
    "these contracts ensures that you can swap data sources without touching\n",
    "downstream code.\n",
    "\n",
    "### Prices DataFrame\n",
    "\n",
    "| Field | Type | Constraints |\n",
    "|-------|------|-------------|\n",
    "| **Index level 0**: `date` | `datetime64[ns]` | Sorted, business days, tz-naive |\n",
    "| **Index level 1**: `ticker` | `str` | Non-empty, consistent casing |\n",
    "| `open` | `float64` | > 0 |\n",
    "| `high` | `float64` | >= `max(open, close)` |\n",
    "| `low` | `float64` | <= `min(open, close)`, > 0 |\n",
    "| `close` | `float64` | > 0 |\n",
    "| `volume` | `float64` or `int64` | >= 0 |\n",
    "| `adj_close` | `float64` | > 0; accounts for splits and dividends |\n",
    "\n",
    "### Signal Series\n",
    "\n",
    "| Field | Type | Constraints |\n",
    "|-------|------|-------------|\n",
    "| **Index level 0**: `date` | `datetime64[ns]` | Matches prices dates |\n",
    "| **Index level 1**: `ticker` | `str` | Subset of universe |\n",
    "| **Values** | `float64` | Finite or NaN (no +/-inf) |\n",
    "\n",
    "### Weights Series\n",
    "\n",
    "| Field | Type | Constraints |\n",
    "|-------|------|-------------|\n",
    "| **Index level 0**: `date` | `datetime64[ns]` | Matches prices dates |\n",
    "| **Index level 1**: `ticker` | `str` | Subset of universe |\n",
    "| **Values** | `float64` | Finite; per-date abs sum is finite |\n",
    "| *Convention* | --- | Positive = long, negative = short |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3da321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that validation catches bad data\n",
    "from qlab.utils.validation import validate_prices, QlabValidationError\n",
    "\n",
    "# --- Test 1: negative close price ---\n",
    "bad_dates = pd.bdate_range(\"2024-01-01\", periods=5)\n",
    "bad_idx = pd.MultiIndex.from_product([bad_dates, [\"BAD\"]], names=[\"date\", \"ticker\"])\n",
    "bad_prices = pd.DataFrame({\n",
    "    \"open\": [100.0, 101, 102, 103, 104],\n",
    "    \"high\": [105.0, 106, 107, 108, 109],\n",
    "    \"low\":  [95.0, 96, 97, 98, 99],\n",
    "    \"close\": [100.0, 101, -1, 103, 104],   # negative close!\n",
    "    \"volume\": [1e6] * 5,\n",
    "    \"adj_close\": [100.0, 101, -1, 103, 104],\n",
    "}, index=bad_idx)\n",
    "\n",
    "try:\n",
    "    validate_prices(bad_prices)\n",
    "except QlabValidationError as e:\n",
    "    print(f\"Test 1 - Caught: {e}\")\n",
    "\n",
    "# --- Test 2: missing required column ---\n",
    "bad_prices2 = bad_prices.drop(columns=[\"volume\"])\n",
    "try:\n",
    "    validate_prices(bad_prices2)\n",
    "except QlabValidationError as e:\n",
    "    print(f\"Test 2 - Caught: {e}\")\n",
    "\n",
    "# --- Test 3: wrong index type ---\n",
    "bad_prices3 = bad_prices.reset_index(drop=True)\n",
    "try:\n",
    "    validate_prices(bad_prices3)\n",
    "except QlabValidationError as e:\n",
    "    print(f\"Test 3 - Caught: {e}\")\n",
    "\n",
    "print(\"\\nAll validation guards work correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f49e1",
   "metadata": {},
   "source": [
    "## 7. Reproducibility Checklist and Recommended Project Layout\n",
    "\n",
    "### Reproducibility checklist\n",
    "\n",
    "| Step | How qlab helps |\n",
    "|------|---------------|\n",
    "| Pin data snapshot | `ParquetCache` writes immutable Parquet files |\n",
    "| Version data files | Commit cache directory or use DVC |\n",
    "| Record provider settings | `inspect.signature` + `__init__` args |\n",
    "| Seed random state | `MockRestApiProvider(seed=42)` |\n",
    "| Lock dependencies | `pip freeze > requirements.txt` |\n",
    "| Log run metadata | `BacktestConfig` is a frozen dataclass |\n",
    "| Timezone consistency | All dates are tz-naive `datetime64[ns]` normalised to midnight |\n",
    "\n",
    "### Recommended project layout\n",
    "\n",
    "```\n",
    "my_research/\n",
    "+-- data/\n",
    "|   +-- raw/                  # ParquetCache output (git-ignored)\n",
    "|   +-- processed/            # Feature matrices\n",
    "|   +-- data_manifest.json    # Records what was fetched and when\n",
    "+-- notebooks/\n",
    "|   +-- 01_exploration.ipynb\n",
    "|   +-- 02_backtest.ipynb\n",
    "+-- src/\n",
    "|   +-- providers/\n",
    "|   |   +-- my_api.py         # Custom MarketDataProvider subclass\n",
    "|   |   +-- news.py           # Custom NewsProvider subclass\n",
    "|   +-- signals/\n",
    "|   |   +-- momentum.py\n",
    "|   |   +-- news_signal.py\n",
    "|   +-- config.py             # Tickers, dates, parameters\n",
    "+-- tests/\n",
    "|   +-- test_providers.py\n",
    "|   +-- test_signals.py\n",
    "+-- pyproject.toml\n",
    "+-- README.md\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how to introspect provider configuration for logging\n",
    "import inspect\n",
    "\n",
    "provider_info = {\n",
    "    \"class\": type(provider).__name__,\n",
    "    \"module\": type(provider).__module__,\n",
    "    \"init_signature\": str(inspect.signature(type(provider).__init__)),\n",
    "}\n",
    "print(\"Provider metadata for reproducibility logging:\")\n",
    "for k, v in provider_info.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Demonstrate determinism of MockRestApiProvider\n",
    "mock_a = MockRestApiProvider(seed=42).fetch([\"X\", \"Y\"], \"2023-01-01\", \"2023-06-30\")\n",
    "mock_b = MockRestApiProvider(seed=42).fetch([\"X\", \"Y\"], \"2023-01-01\", \"2023-06-30\")\n",
    "print(f\"\\nDeterminism check: same seed produces identical data = {mock_a.equals(mock_b)}\")\n",
    "\n",
    "mock_c = MockRestApiProvider(seed=99).fetch([\"X\", \"Y\"], \"2023-01-01\", \"2023-06-30\")\n",
    "print(f\"Different seed produces different data = {not mock_a.equals(mock_c)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9396eeb0",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "| Topic | Key takeaway |\n",
    "|-------|-------------|\n",
    "| `MarketDataProvider` | Single abstract method `fetch()` --- implement once, use everywhere |\n",
    "| `YFinanceProvider` | Production-ready provider for US equities via yfinance |\n",
    "| Custom providers | Subclass `MarketDataProvider`, return stacked `(date, ticker)` DataFrame |\n",
    "| `ParquetCache` | Wrap any provider for reproducibility and speed |\n",
    "| Alternative data | Define your own ABCs (e.g. `NewsProvider`); aggregate to daily `(date, ticker)` signals |\n",
    "| Data contracts | Prices: 6 OHLCV+adj columns; Signals: float Series; Weights: finite float Series |\n",
    "| Reproducibility | Pin data, seed RNG, log configs, version everything |\n",
    "\n",
    "### Next steps\n",
    "\n",
    "* **Tutorial 6** --- Build a multi-factor model combining price and alternative-data signals.\n",
    "* Try connecting a real API (Alpha Vantage, Polygon.io, Databento) by\n",
    "  subclassing `MarketDataProvider`.\n",
    "* Experiment with different cache backends (SQLite, HDF5) by following\n",
    "  the `ParquetCache` pattern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
